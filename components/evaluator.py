from typing import List
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from prompts import system_prompts


class GradeDocuments(BaseModel):
    """Binary score for relevance check on retrieved documents."""
    binary_score: str = Field(
        description="Documents are relevant to the question, 'yes' or 'no'"
    )

class GradeHallucinations(BaseModel):
    """Binary score for hallucination present in generation answer."""
    binary_score: str = Field(
        description="Answer is grounded in the facts, 'yes' or 'no'"
    )

class Evaluator:
    """ë¬¸ì„œ ë° ë‹µë³€ í‰ê°€ ë‹´ë‹¹ í´ë˜ìŠ¤"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self._setup_graders()
    
    def _setup_graders(self):
        """í‰ê°€ê¸° ì„¤ì •"""
        # ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ê¸°
        self.structured_llm_grader = self.llm.with_structured_output(GradeDocuments, method="function_calling")
        self.grade_prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompts.get("GRADER")),
            ("human", "Retrieved document: \n\n {document} \n\n User question: {question}"),
        ])
        self.retrieval_grader = self.grade_prompt | self.structured_llm_grader
        
        # í• ë£¨ì‹œë„¤ì´ì…˜ í‰ê°€ê¸°
        self.hallucination_grader_llm = self.llm.with_structured_output(GradeHallucinations, method="function_calling")
        self.hallucination_prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompts.get("HALLUCINATION")),
            ("human", "Set of facts: \n\n {documents} \n\n LLM generation: {generation} \n\n Question: {question}"),
        ])
        self.hallucination_grader = self.hallucination_prompt | self.hallucination_grader_llm
    
    def grade_documents(self, question: str, documents: List[Document]) -> List[Document]:
        """ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤."""
        print("==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====")
        filtered_docs = []
        
        for d in documents:
            try:
                score = self.retrieval_grader.invoke(
                    {"question": question, "document": d.page_content}
                )
                grade = score.binary_score
                if grade.lower() == "yes":
                    print(f"---GRADE: DOCUMENT RELEVANT--- (Score: {d.metadata.get('similarity_score', 'N/A')})")
                    filtered_docs.append(d)
                else:
                    print(f"---GRADE: DOCUMENT NOT RELEVANT--- (Score: {d.metadata.get('similarity_score', 'N/A')})")
            except Exception as e:
                print(f"---ERROR GRADING DOCUMENT: {str(e)}---")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¼ë‹¨ í¬í•¨ (ì•ˆì „ì„ ìœ„í•´)
                filtered_docs.append(d)
                
        print(f"  ğŸ“„ ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œ: {len(filtered_docs)}/{len(documents)}ê°œ")
        return filtered_docs
    
    def check_hallucination(self, documents: List[Document], generation: str, question: str) -> str:
        """ìƒì„±ëœ ë‹µë³€ì˜ í• ë£¨ì‹œë„¤ì´ì…˜ ì—¬ë¶€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤."""
        print("==== [CHECK HALLUCINATIONS] ====")
        
        if not documents:
            print("  âš ï¸ í‰ê°€í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤ - í™˜ê° ê²€ì‚¬ ìƒëµ")
            return "relevant"  # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ê²€ì‚¬ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ í†µê³¼ì‹œí‚´
        
        # ë” êµ¬ì¡°í™”ëœ ë¬¸ì„œ í˜•ì‹í™”
        formatted_docs = self._format_documents_for_evaluation(documents)
        
        try:
            # í™˜ê° í‰ê°€
            score = self.hallucination_grader.invoke({
                "documents": formatted_docs, 
                "generation": generation,
                "question": question
            })
            
            grade = score.binary_score.lower()
            
            if grade == "yes":
                print("==== [DECISION: ANSWER IS GROUNDED IN DOCUMENTS] ====")
                return "relevant"
            else:
                print("==== [DECISION: HALLUCINATION DETECTED] ====")
                return "hallucination"
                
        except Exception as e:
            print(f"==== [HALLUCINATION CHECK ERROR: {str(e)}] ====")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            print("  âš ï¸ í™˜ê° ê°ì§€ ì‹¤íŒ¨ - ì•ˆì „ì„ ìœ„í•´ ì¬ìƒì„± ì§„í–‰")
            return "hallucination"
    
    def _format_documents_for_evaluation(self, documents: List[Document]) -> str:
        """í‰ê°€ìš© ë¬¸ì„œ í˜•ì‹í™” - ë” ìƒì„¸í•œ ì†ŒìŠ¤ ì •ë³´ í¬í•¨"""
        formatted_docs = []
        
        for i, doc in enumerate(documents):
            # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
            source_type = doc.metadata.get("source_type", "unknown")
            source = doc.metadata.get("source", "unknown")
            title = doc.metadata.get("title", "ì œëª© ì—†ìŒ")
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ìˆëŠ” ê²½ìš°)
            authors = doc.metadata.get("authors", "")
            if isinstance(authors, list):
                authors = ", ".join(authors)
            
            year = doc.metadata.get("year", "")
            journal = doc.metadata.get("journal", "")
            similarity = doc.metadata.get("similarity_score", "")
            
            # ë¬¸ì„œ ë²ˆí˜¸ì™€ ì†ŒìŠ¤ íƒ€ì…ìœ¼ë¡œ ì‹œì‘
            formatted_docs.append(f"--- DOCUMENT {i+1} [{source_type.upper()}] ---")
            formatted_docs.append(f"TITLE: {title}")
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ)
            if authors:
                formatted_docs.append(f"AUTHORS: {authors}")
            if year:
                formatted_docs.append(f"YEAR: {year}")
            if journal:
                formatted_docs.append(f"JOURNAL: {journal}")
            if similarity:
                formatted_docs.append(f"RELEVANCE: {similarity:.4f}")
            
            formatted_docs.append(f"SOURCE: {source}")
            formatted_docs.append("CONTENT:")
            formatted_docs.append(doc.page_content)
            formatted_docs.append("---")
        
        return "\n".join(formatted_docs)