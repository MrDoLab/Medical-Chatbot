# components/text_processor.py
"""
í…ìŠ¤íŠ¸ íŒŒì¼ ì „ìš© ì²˜ë¦¬ê¸° - .txt, .md, .json ë“±
"""

import json
import re
from typing import Dict, List
from pathlib import Path
from langchain_core.documents import Document
from datetime import datetime

class TextProcessor:
    """í…ìŠ¤íŠ¸ íŒŒì¼ ì „ìš© ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        """í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”"""
        self.max_content_length = 8000  # í† í° ì œí•œ
        self.supported_extensions = ['.txt', '.md', '.json']
        
        print("ğŸ“ í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def process_text_file(self, file_path: Path) -> Document:
        """í…ìŠ¤íŠ¸ íŒŒì¼ì„ Document ê°ì²´ë¡œ ë³€í™˜"""
        print(f"    ğŸ“ í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬: {file_path.name}")
        
        try:
            extension = file_path.suffix.lower()
            
            if extension == '.json':
                content = self._process_json_file(file_path)
            elif extension == '.md':
                content = self._process_markdown_file(file_path)
            elif extension == '.txt':
                content = self._process_txt_file(file_path)
            else:
                # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                content = self._process_txt_file(file_path)
            
            if not content or len(content.strip()) < 20:
                return self._create_empty_document(file_path, "ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŒ")
            
            # í† í° ì œí•œ ì ìš©
            if len(content) > self.max_content_length:
                content = content[:self.max_content_length] + "\n\n[ë‚´ìš©ì´ ê¸¸ì–´ ì¼ë¶€ ìƒëµë¨]"
            
            print(f"    âœ… í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ: {len(content)}ì")
            
            return Document(
                page_content=content,
                metadata={
                    "source": str(file_path),
                    "title": file_path.stem,
                    "file_type": extension[1:],  # ì  ì œê±°
                    "processed_at": datetime.now().isoformat(),
                    "category": self._infer_category_from_filename(file_path.name),
                    "content_length": len(content),
                    "original_length": len(content) if len(content) <= self.max_content_length else "truncated"
                }
            )
            
        except Exception as e:
            print(f"    âŒ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return self._create_error_document(file_path, str(e))
    
    def _process_txt_file(self, file_path: Path) -> str:
        """ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬"""
        try:
            # ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
            encodings = ['utf-8', 'cp949', 'euc-kr', 'latin-1']
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                    
                    # ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìœ¼ë©´ í…ìŠ¤íŠ¸ ì •ë¦¬ í›„ ë°˜í™˜
                    return self._clean_text_content(content)
                    
                except UnicodeDecodeError:
                    continue
            
            # ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨
            print(f"    âš ï¸ ì¸ì½”ë”© ê°ì§€ ì‹¤íŒ¨: {file_path.name}")
            return f"íŒŒì¼ ì¸ì½”ë”©ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path.name}"
            
        except Exception as e:
            return f"í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}"
    
    def _process_markdown_file(self, file_path: Path) -> str:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì²˜ë¦¬"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ë§ˆí¬ë‹¤ìš´ íŠ¹í™” ì²˜ë¦¬
            cleaned_content = self._clean_markdown_content(content)
            return cleaned_content
            
        except Exception as e:
            return f"ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
    
    def _process_json_file(self, file_path: Path) -> str:
        """JSON íŒŒì¼ ì²˜ë¦¬"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # JSON êµ¬ì¡°ì— ë”°ë¥¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if isinstance(data, list):
                return self._extract_from_json_list(data)
            elif isinstance(data, dict):
                return self._extract_from_json_dict(data)
            else:
                return str(data)
                
        except json.JSONDecodeError as e:
            return f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}"
        except Exception as e:
            return f"JSON íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
    
    def _clean_text_content(self, content: str) -> str:
        """ì¼ë°˜ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not content:
            return ""
        
        # ê¸°ë³¸ ì •ë¦¬
        content = content.strip()
        
        # ê³¼ë„í•œ ê³µë°± ì •ë¦¬
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)  # 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ
        content = re.sub(r' +', ' ', content)  # ì—¬ëŸ¬ ê³µë°±ì„ 1ê°œë¡œ
        content = re.sub(r'\t+', ' ', content)  # íƒ­ì„ ê³µë°±ìœ¼ë¡œ
        
        # BOM ì œê±°
        content = content.replace('\ufeff', '')
        
        return content
    
    def _clean_markdown_content(self, content: str) -> str:
        """ë§ˆí¬ë‹¤ìš´ ë‚´ìš© ì •ë¦¬"""
        if not content:
            return ""
        
        # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ë¦¬
        content = self._clean_text_content(content)
        
        # ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ê°„ì†Œí™” (ì™„ì „ ì œê±°í•˜ì§€ ì•Šê³  ì½ê¸° ì‰½ê²Œ)
        content = re.sub(r'^#{1,6}\s+', 'â–  ', content, flags=re.MULTILINE)  # í—¤ë”©ì„ â– ë¡œ
        content = re.sub(r'\*\*(.*?)\*\*', r'ã€\1ã€‘', content)  # ë³¼ë“œë¥¼ ã€ã€‘ë¡œ
        content = re.sub(r'\*(.*?)\*', r'ã€ˆ\1ã€‰', content)  # ì´íƒ¤ë¦­ì„ ã€ˆã€‰ë¡œ
        content = re.sub(r'`(.*?)`', r'"\1"', content)  # ì½”ë“œë¥¼ ""ë¡œ
        
        # ë§í¬ ì •ë¦¬
        content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)  # [í…ìŠ¤íŠ¸](ë§í¬) â†’ í…ìŠ¤íŠ¸
        
        # ë¦¬ìŠ¤íŠ¸ ë§ˆì»¤ ì •ë¦¬
        content = re.sub(r'^[\-\*\+]\s+', 'â€¢ ', content, flags=re.MULTILINE)
        content = re.sub(r'^\d+\.\s+', '1. ', content, flags=re.MULTILINE)
        
        return content
    
    def _extract_from_json_list(self, data: List) -> str:
        """JSON ë¦¬ìŠ¤íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        content_parts = []
        
        for i, item in enumerate(data):
            if isinstance(item, dict):
                # ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ í•„ë“œë“¤ ì°¾ê¸°
                text_fields = ['content', 'text', 'description', 'body', 'message', 'title']
                
                for field in text_fields:
                    if field in item and isinstance(item[field], str):
                        content_parts.append(f"[í•­ëª© {i+1}] {item[field]}")
                        break
                else:
                    # í…ìŠ¤íŠ¸ í•„ë“œê°€ ì—†ìœ¼ë©´ ì „ì²´ JSONì„ ë¬¸ìì—´ë¡œ
                    content_parts.append(f"[í•­ëª© {i+1}] {json.dumps(item, ensure_ascii=False, indent=2)}")
            
            elif isinstance(item, str):
                content_parts.append(f"[í•­ëª© {i+1}] {item}")
            
            else:
                content_parts.append(f"[í•­ëª© {i+1}] {str(item)}")
        
        return "\n\n".join(content_parts)
    
    def _extract_from_json_dict(self, data: Dict) -> str:
        """JSON ë”•ì…”ë„ˆë¦¬ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        # ì˜ë£Œ ë¬¸ì„œì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” í•„ë“œëª…ë“¤
        priority_fields = [
            'content', 'text', 'description', 'body', 'message',
            'title', 'summary', 'abstract', 'procedure', 'symptoms',
            'treatment', 'diagnosis', 'medication', 'dosage'
        ]
        
        content_parts = []
        
        # ìš°ì„ ìˆœìœ„ í•„ë“œ ë¨¼ì € ì²˜ë¦¬
        for field in priority_fields:
            if field in data and isinstance(data[field], str) and data[field].strip():
                content_parts.append(f"{field.upper()}: {data[field]}")
        
        # ë‚˜ë¨¸ì§€ í•„ë“œë“¤ ì²˜ë¦¬
        for key, value in data.items():
            if key not in priority_fields:
                if isinstance(value, str) and value.strip():
                    content_parts.append(f"{key}: {value}")
                elif isinstance(value, (list, dict)):
                    content_parts.append(f"{key}: {json.dumps(value, ensure_ascii=False, indent=2)}")
                else:
                    content_parts.append(f"{key}: {str(value)}")
        
        return "\n\n".join(content_parts)
    
    def _infer_category_from_filename(self, filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ """
        filename_lower = filename.lower()
        
        category_keywords = {
            "ì‘ê¸‰ì²˜ì¹˜": ["ì‘ê¸‰", "emergency", "ê¸‰ì„±", "ìœ„ê¸‰", "êµ¬ê¸‰"],
            "ì•½ë¬¼ì •ë³´": ["ì•½ë¬¼", "drug", "medication", "ì²˜ë°©", "pharmacology"],
            "ì§„ë‹¨": ["ì§„ë‹¨", "diagnosis", "ê²€ì‚¬", "test", "examination"],
            "ì¹˜ë£Œ": ["ì¹˜ë£Œ", "treatment", "therapy", "ìˆ˜ìˆ ", "surgery"],
            "ê°„í˜¸": ["ê°„í˜¸", "nursing", "care", "ëŒë´„"],
            "ë‚´ê³¼": ["ë‚´ê³¼", "internal", "ìˆœí™˜ê¸°", "í˜¸í¡ê¸°", "ì†Œí™”ê¸°"],
            "ì™¸ê³¼": ["ì™¸ê³¼", "surgery", "ìˆ˜ìˆ ", "ì •í˜•ì™¸ê³¼", "ì‹ ê²½ì™¸ê³¼"],
            "ì†Œì•„ê³¼": ["ì†Œì•„", "pediatric", "ì•„ë™", "ì‹ ìƒì•„"],
            "ì‚°ë¶€ì¸ê³¼": ["ì‚°ë¶€ì¸ê³¼", "obstetrics", "gynecology", "ì„ì‹ ", "ì¶œì‚°"],
            "ê°€ì´ë“œë¼ì¸": ["guideline", "ê°€ì´ë“œ", "ì§€ì¹¨", "í”„ë¡œí† ì½œ", "protocol"],
            "ë§¤ë‰´ì–¼": ["manual", "ë§¤ë‰´ì–¼", "ì•ˆë‚´ì„œ", "handbook"]
        }
        
        for category, keywords in category_keywords.items():
            if any(keyword in filename_lower for keyword in keywords):
                return category
        
        return "ì¼ë°˜ì˜í•™"
    
    def _create_empty_document(self, file_path: Path, reason: str) -> Document:
        """ë¹ˆ ë¬¸ì„œ ì²˜ë¦¬ìš© Document"""
        return Document(
            page_content=f"""
íŒŒì¼ëª…: {file_path.name}
ìƒíƒœ: ë‚´ìš© ì—†ìŒ

ì´ìœ : {reason}

ì´ íŒŒì¼ì€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.
íŒŒì¼ì„ í™•ì¸í•˜ê³  ë‚´ìš©ì„ ì¶”ê°€í•œ í›„ ë‹¤ì‹œ ì—…ë¡œë“œí•˜ì„¸ìš”.
""",
            metadata={
                "source": str(file_path),
                "title": file_path.stem,
                "file_type": file_path.suffix[1:],
                "processed_at": datetime.now().isoformat(),
                "category": "ë¹ˆíŒŒì¼",
                "status": "empty",
                "reason": reason
            }
        )
    
    def _create_error_document(self, file_path: Path, error: str) -> Document:
        """ì—ëŸ¬ ì²˜ë¦¬ìš© Document"""
        return Document(
            page_content=f"""
íŒŒì¼ëª…: {file_path.name}
ìƒíƒœ: ì²˜ë¦¬ ì‹¤íŒ¨

ì˜¤ë¥˜: {error}

ì´ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.
íŒŒì¼ í˜•ì‹ì´ë‚˜ ì¸ì½”ë”©ì„ í™•ì¸í•´ë³´ì„¸ìš”.

ì§€ì›ë˜ëŠ” í˜•ì‹: .txt, .md, .json
ì§€ì›ë˜ëŠ” ì¸ì½”ë”©: UTF-8, CP949, EUC-KR
""",
            metadata={
                "source": str(file_path),
                "title": file_path.stem,
                "file_type": file_path.suffix[1:],
                "processed_at": datetime.now().isoformat(),
                "category": "ì²˜ë¦¬ì‹¤íŒ¨",
                "status": "error",
                "error": error
            }
        )
    
    def is_supported_file(self, file_path: Path) -> bool:
        """ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ì¸ì§€ í™•ì¸"""
        return file_path.suffix.lower() in self.supported_extensions
    
    def get_stats(self) -> Dict:
        """í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸° í†µê³„"""
        return {
            "processor_type": "TextProcessor",
            "supported_extensions": self.supported_extensions,
            "max_content_length": self.max_content_length
        }