# components/medgemma_searcher.py
from typing import List, Dict, Any, Optional
from langchain_core.documents import Document
from datetime import datetime
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import logging

logger = logging.getLogger(__name__)

class MedGemmaSearcher:
    """MedGemma ì˜ë£Œ íŠ¹í™” LLM ê²€ìƒ‰ ë‹´ë‹¹ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = "microsoft/DialoGPT-medium", device: str = "auto"):
        """
        MedGemma ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  Gemma ëª¨ë¸ëª… (ì˜ë£Œ íŒŒì¸íŠœë‹ ë²„ì „ ê¶Œì¥)
            device: ì‹¤í–‰ ë””ë°”ì´ìŠ¤ ("auto", "cpu", "cuda")
        """
        self.model_name = model_name
        self.device = self._get_device(device)
        
        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        
        # ì˜ë£Œ íŠ¹í™” ì„¤ì •
        self.medical_system_prompt = """You are MedGemma, a medical AI assistant specialized in providing accurate, evidence-based medical information for healthcare professionals.

Your capabilities:
- Provide clinical guidelines and treatment protocols
- Explain medical procedures and diagnostic criteria
- Offer drug information including dosages and contraindications
- Support emergency response protocols
- Give differential diagnosis suggestions

Guidelines:
- Always prioritize patient safety
- Use precise medical terminology
- Include relevant contraindications and warnings
- Mention when to seek immediate medical attention
- Provide step-by-step clinical procedures when appropriate
- Always respond in Korean regardless of input language

Remember: You are providing information for medical professionals. Be thorough and clinically oriented."""

        # ëª¨ë¸ ë¡œë“œ ì‹œë„
        self.model_loaded = False
        self._try_load_model()
        
        # ê²€ìƒ‰ í†µê³„
        self.search_stats = {
            "queries_processed": 0,
            "successful_generations": 0,
            "failed_generations": 0,
            "average_response_length": 0,
            "total_tokens_generated": 0
        }
    
    def _get_device(self, device: str) -> str:
        """ìµœì ì˜ ë””ë°”ì´ìŠ¤ ê²°ì •"""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            elif torch.backends.mps.is_available():  # Apple Silicon
                return "mps"
            else:
                return "cpu"
        return device
    
    def _try_load_model(self):
        """MedGemma ëª¨ë¸ ë¡œë“œ ì‹œë„"""
        print(f"ğŸ§  MedGemma ëª¨ë¸ ë¡œë”© ì¤‘... ({self.device})")
        
        import os
        is_cloud = os.environ.get('STREAMLIT_SHARING', '') == 'true'

        # í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œëŠ” ëª¨ë¸ ë¡œë”© ê±´ë„ˆëœ€
        if is_cloud:
            print("âš ï¸ Streamlit Cloud í™˜ê²½ ê°ì§€: MedGemma ëª¨ë¸ ë¡œë”© ê±´ë„ˆëœ€")
            self.model_loaded = False
            return

        try:
            # Hugging Face í† í° ë¡œë“œ
            import os
            from dotenv import load_dotenv
            load_dotenv()
            
            hf_token = os.getenv("HUGGINGFACE_HUB_TOKEN")
            if hf_token:
                print("âœ… Hugging Face í† í° í™•ì¸ë¨")
            else:
                print("âš ï¸ Hugging Face í† í°ì´ ì—†ìŠµë‹ˆë‹¤")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                token=hf_token  # í† í° ëª…ì‹œì  ì „ë‹¬
            )
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ëª¨ë¸ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                token=hf_token  # í† í° ëª…ì‹œì  ì „ë‹¬
            )
            
            # íŒŒì´í”„ë¼ì¸ ìƒì„±
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=self.device if self.device != "auto" else None,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                trust_remote_code=True
            )
            
            self.model_loaded = True
            print(f"âœ… MedGemma ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({self.device})")
            
        except Exception as e:
            logger.error(f"MedGemma ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            print(f"âŒ MedGemma ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            self.model_loaded = False
    
    def search_medgemma(self, query: str, max_results: int = 3, max_length: int = 512) -> List[Document]:
        """MedGemmaë¥¼ ì‚¬ìš©í•œ ì˜ë£Œ ì§€ì‹ ê²€ìƒ‰ (PubMedSearcher íŒ¨í„´ í˜¸í™˜)"""
        print(f"==== [MEDGEMMA SEARCH: {query}] ====")
        
        self.search_stats["queries_processed"] += 1
        
        if not self.model_loaded:
            print("  âŒ MedGemma ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            return self._create_fallback_documents(query)
        
        try:
            # ì˜ë£Œ íŠ¹í™” í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            medical_prompt = self._build_medical_prompt(query)
            
            # MedGemma ì¶”ë¡  ì‹¤í–‰
            response = self._generate_medical_response(medical_prompt, max_length)
            
            if response and len(response.strip()) > 10:  # ìµœì†Œ ê¸¸ì´ í™•ì¸
                # Document ê°ì²´ë¡œ ë³€í™˜
                document = self._convert_to_document(query, response)
                
                self.search_stats["successful_generations"] += 1
                self.search_stats["total_tokens_generated"] += len(response.split())
                
                print(f"  âœ… MedGemma ì‘ë‹µ ìƒì„± ì™„ë£Œ ({len(response)}ì)")
                return [document]
            else:
                print(f"  âŒ MedGemma ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ: '{response}'")
                self.search_stats["failed_generations"] += 1
                return self._create_fallback_documents(query)
                
        except Exception as e:
            logger.error(f"MedGemma ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            print(f"  âŒ MedGemma ì˜¤ë¥˜: {str(e)}")
            self.search_stats["failed_generations"] += 1
            return self._create_fallback_documents(query)
    
    def _build_medical_prompt(self, query: str) -> str:
        """ì˜ë£Œ ì§ˆë¬¸ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ë‹¨ìˆœí™”)"""
        
        # ë‹¨ìˆœí•œ í”„ë¡¬í”„íŠ¸ë¡œ ë³€ê²½
        prompt = f"""ë‹¤ìŒì€ ì˜ë£Œì§„ì„ ìœ„í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤. ìƒì„¸í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query}

ë‹µë³€:"""
        
        return prompt
        
        return prompt
    
    def _detect_medical_question_type(self, query: str) -> str:
        """ì˜ë£Œ ì§ˆë¬¸ ìœ í˜• ê°ì§€"""
        query_lower = query.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        if any(word in query_lower for word in ["ì‘ê¸‰", "ê¸‰ì„±", "ì‹¬ì •ì§€", "ì‡¼í¬", "ì¶œí˜ˆ"]):
            return "emergency"
        elif any(word in query_lower for word in ["ì§„ë‹¨", "ê²€ì‚¬", "ì¦ìƒ", "ì›ì¸"]):
            return "diagnosis"
        elif any(word in query_lower for word in ["ì¹˜ë£Œ", "ì²˜ì¹˜", "ê´€ë¦¬", "ìš”ë²•"]):
            return "treatment"
        elif any(word in query_lower for word in ["ì•½ë¬¼", "ì•½", "ì²˜ë°©", "ë¶€ì‘ìš©"]):
            return "medication"
        elif any(word in query_lower for word in ["ìˆ˜ìˆ ", "ì‹œìˆ ", "ì ˆì°¨", "í”„ë¡œí† ì½œ"]):
            return "procedure"
        else:
            return "general"
    
    def _generate_medical_response(self, prompt: str, max_length: int) -> Optional[str]:
        """MedGemmaë¥¼ ì‚¬ìš©í•œ ì˜ë£Œ ì‘ë‹µ ìƒì„±"""
        
        try:
            # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì • (ë§¤ìš° ê´€ëŒ€í•˜ê²Œ)
            generation_config = {
                "max_new_tokens": max_length,
                "min_length": 20,  # ìµœì†Œ ê¸¸ì´ ì¤„ì„
                "temperature": 0.7,  # ë” ì°½ì˜ì ìœ¼ë¡œ
                "top_p": 0.9,
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "repetition_penalty": 1.1
            }
            
            print(f"    ğŸ¤– MedGemma ì¶”ë¡  ì‹œì‘... (max_tokens: {max_length})")
            print(f"    ğŸ“‹ í”„ë¡¬í”„íŠ¸: '{prompt[:200]}...'")
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë¨¼ì €
            try:
                test_output = self.pipeline("ì•ˆë…•í•˜ì„¸ìš”", max_new_tokens=10, do_sample=False)
                print(f"    ğŸ§ª ê¸°ë³¸ í…ŒìŠ¤íŠ¸: '{test_output[0]['generated_text']}'")
            except Exception as e:
                print(f"    âŒ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            
            # ì‘ë‹µ ìƒì„±
            outputs = self.pipeline(
                prompt,
                **generation_config,
                return_full_text=False
            )
            
            print(f"    ğŸ” íŒŒì´í”„ë¼ì¸ ì¶œë ¥ íƒ€ì…: {type(outputs)}")
            print(f"    ğŸ” íŒŒì´í”„ë¼ì¸ ì¶œë ¥ ê¸¸ì´: {len(outputs) if outputs else 0}")
            
            if outputs and len(outputs) > 0:
                generated_text = outputs[0]["generated_text"]
                print(f"    ğŸ“ ì›ë³¸ ì‘ë‹µ: '{generated_text[:100]}...'")
                print(f"    ğŸ“ ì›ë³¸ ì‘ë‹µ ê¸¸ì´: {len(generated_text)}ì")
                
                # ì‘ë‹µ í›„ì²˜ë¦¬
                cleaned_response = self._clean_medical_response(generated_text)
                print(f"    âœ¨ ì •ë¦¬ëœ ì‘ë‹µ ê¸¸ì´: {len(cleaned_response)}ì")
                
                return cleaned_response
            else:
                print(f"    âŒ íŒŒì´í”„ë¼ì¸ ì¶œë ¥ì´ ë¹„ì–´ìˆìŒ")
                return None
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            print(f"    âŒ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _clean_medical_response(self, response: str) -> str:
        """ìƒì„±ëœ ì˜ë£Œ ì‘ë‹µ ì •ë¦¬"""
        
        # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
        clean_response = response.strip()
        
        # ë°˜ë³µ íŒ¨í„´ ì œê±°
        lines = clean_response.split('\n')
        unique_lines = []
        seen_lines = set()
        
        for line in lines:
            line_clean = line.strip()
            if line_clean and line_clean not in seen_lines:
                unique_lines.append(line)
                seen_lines.add(line_clean)
        
        clean_response = '\n'.join(unique_lines)
        
        # ì˜ë£Œ ì •ë³´ ê²€ì¦ ë§ˆí¬ ì¶”ê°€
        if len(clean_response) > 50:
            clean_response += "\n\nâš ï¸ ì´ ì •ë³´ëŠ” AIê°€ ìƒì„±í•œ ê²ƒìœ¼ë¡œ, ì‹¤ì œ ì§„ë£Œ ì‹œì—ëŠ” ë°˜ë“œì‹œ ì˜ë£Œì§„ê³¼ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        
        return clean_response
    
    def _convert_to_document(self, query: str, response: str) -> Document:
        """MedGemma ì‘ë‹µì„ Document ê°ì²´ë¡œ ë³€í™˜"""
        
        # ì‘ë‹µ í’ˆì§ˆ í‰ê°€
        quality_score = self._assess_response_quality(response)
        
        # ì˜ë£Œ ì¹´í…Œê³ ë¦¬ ì¶”ì •
        estimated_category = self._estimate_medical_category(query, response)
        
        # Document ìƒì„±
        content = f"""MedGemma ì˜ë£Œ ì§€ì‹ ì‘ë‹µ:

ì§ˆë¬¸: {query}

ë‹µë³€:
{response}

ìƒì„± ì •ë³´:
- ëª¨ë¸: {self.model_name}
- ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- í’ˆì§ˆ ì ìˆ˜: {quality_score}/10
"""

        metadata = {
            "source": f"medgemma_{self.model_name}",
            "source_type": "medgemma",
            "model_name": self.model_name,
            "query": query,
            "generated_at": datetime.now().isoformat(),
            "device": self.device,
            "quality_score": quality_score,
            "estimated_category": estimated_category,
            "response_length": len(response),
            "reliability": "high",  # MedGemmaëŠ” ì˜ë£Œ íŠ¹í™” ëª¨ë¸
            "confidence": "high" if quality_score >= 7 else "medium"
        }
        
        return Document(page_content=content, metadata=metadata)
    
    def _assess_response_quality(self, response: str) -> float:
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€ (1-10ì )"""
        score = 5.0  # ê¸°ë³¸ ì ìˆ˜
        
        # ê¸¸ì´ í‰ê°€
        if 100 <= len(response) <= 1000:
            score += 1.0
        elif len(response) > 1000:
            score += 0.5
        
        # ì˜ë£Œ ìš©ì–´ í¬í•¨ ì—¬ë¶€
        medical_terms = ["ì¹˜ë£Œ", "ì§„ë‹¨", "ì¦ìƒ", "ì•½ë¬¼", "ì²˜ë°©", "í™˜ì", "ì˜ë£Œì§„", "ë³‘ì›"]
        term_count = sum(1 for term in medical_terms if term in response)
        score += min(2.0, term_count * 0.3)
        
        # êµ¬ì¡°í™”ëœ ì •ë³´ ì—¬ë¶€ (ë²ˆí˜¸, ë‹¨ê³„ ë“±)
        if any(pattern in response for pattern in ["1.", "2.", "ì²«ì§¸", "ë‘˜ì§¸", "ë‹¨ê³„"]):
            score += 1.0
        
        # ì•ˆì „ ì •ë³´ í¬í•¨ ì—¬ë¶€
        if any(word in response for word in ["ì£¼ì˜", "ê²½ê³ ", "ë¶€ì‘ìš©", "ê¸ˆê¸°"]):
            score += 1.0
        
        return min(10.0, score)
    
    def _estimate_medical_category(self, query: str, response: str) -> str:
        """ì˜ë£Œ ì¹´í…Œê³ ë¦¬ ì¶”ì •"""
        combined_text = f"{query} {response}".lower()
        
        category_keywords = {
            "ì‘ê¸‰ì²˜ì¹˜": ["ì‘ê¸‰", "ê¸‰ì„±", "ì‹¬ì •ì§€", "ì‘ê¸‰ì²˜ì¹˜"],
            "ë‚´ê³¼": ["ë‹¹ë‡¨", "ê³ í˜ˆì••", "ë‚´ê³¼", "ë§Œì„±ì§ˆí™˜"],
            "ì™¸ê³¼": ["ìˆ˜ìˆ ", "ì™¸ê³¼", "ì ˆê°œ", "ë´‰í•©"],
            "ì•½ë¬¼ì •ë³´": ["ì•½ë¬¼", "ì²˜ë°©", "ë¶€ì‘ìš©", "ìš©ë²•"],
            "ì§„ë‹¨ê²€ì‚¬": ["ì§„ë‹¨", "ê²€ì‚¬", "ì˜ìƒ", "í˜ˆì•¡"],
            "ê°ì—¼ê´€ë¦¬": ["ê°ì—¼", "í•­ìƒì œ", "ë°”ì´ëŸ¬ìŠ¤", "ì„¸ê· "]
        }
        
        max_matches = 0
        best_category = "ì¼ë°˜ì˜í•™"
        
        for category, keywords in category_keywords.items():
            matches = sum(1 for keyword in keywords if keyword in combined_text)
            if matches > max_matches:
                max_matches = matches
                best_category = category
        
        return best_category
    
    def _create_fallback_documents(self, query: str) -> List[Document]:
        """MedGemma ì‹¤íŒ¨ ì‹œ í´ë°± ë¬¸ì„œ ìƒì„±"""
        fallback_content = f"""
MedGemma ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ ì˜ë£Œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì§ˆë¬¸: {query}

ì¼ë°˜ì ì¸ ì˜ë£Œ ê°€ì´ë“œë¼ì¸:
1. ì •í™•í•œ ì§„ë‹¨ì„ ìœ„í•´ì„œëŠ” ì˜ë£Œì§„ê³¼ ì§ì ‘ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤
2. ì‘ê¸‰ìƒí™© ì‹œì—ëŠ” ì¦‰ì‹œ 119ì— ì‹ ê³ í•˜ì„¸ìš”
3. ì•½ë¬¼ ë³µìš© ì „ì—ëŠ” ë°˜ë“œì‹œ ì „ë¬¸ì˜ì™€ ìƒì˜í•˜ì„¸ìš”
4. ì¦ìƒì´ ì§€ì†ë˜ê±°ë‚˜ ì•…í™”ë˜ë©´ ë³‘ì› ë°©ë¬¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤

âš ï¸ ì´ëŠ” MedGemma ëª¨ë¸ ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ ì•ˆë‚´ì‚¬í•­ì…ë‹ˆë‹¤.
ì •í™•í•œ ì˜ë£Œ ì •ë³´ëŠ” ì˜ë£Œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.
"""
        
        return [Document(
            page_content=fallback_content,
            metadata={
                "source": "medgemma_fallback",
                "source_type": "medgemma",
                "query": query,
                "fallback_reason": "model_unavailable",
                "reliability": "low",
                "confidence": "low",
                "generated_at": datetime.now().isoformat()
            }
        )]
    
    def get_stats(self) -> Dict[str, Any]:
        """MedGemma ê²€ìƒ‰ê¸° í†µê³„"""
        success_rate = 0
        if self.search_stats["queries_processed"] > 0:
            success_rate = self.search_stats["successful_generations"] / self.search_stats["queries_processed"]
        
        avg_length = 0
        if self.search_stats["successful_generations"] > 0:
            avg_length = self.search_stats["total_tokens_generated"] / self.search_stats["successful_generations"]
        
        return {
            "searcher_type": "MedGemmaSearcher",
            "model_info": {
                "model_name": self.model_name,
                "device": self.device,
                "model_loaded": self.model_loaded
            },
            "performance": {
                "queries_processed": self.search_stats["queries_processed"],
                "successful_generations": self.search_stats["successful_generations"],
                "failed_generations": self.search_stats["failed_generations"],
                "success_rate": round(success_rate * 100, 2),
                "average_response_length": round(avg_length, 1)
            },
            "resource_usage": {
                "total_tokens_generated": self.search_stats["total_tokens_generated"],
                "estimated_cost_usd": 0.0  # ë¡œì»¬ ì‹¤í–‰ì´ë¯€ë¡œ ë¹„ìš© ì—†ìŒ
            }
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.model is not None:
                del self.model
            if self.tokenizer is not None:
                del self.tokenizer
            if self.pipeline is not None:
                del self.pipeline
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            print("ğŸ—‘ï¸ MedGemma ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
def test_medgemma_searcher():
    """MedGemma ê²€ìƒ‰ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª MedGemma ê²€ìƒ‰ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    try:
        # ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        searcher = MedGemmaSearcher()
        
        if not searcher.model_loaded:
            print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ë¡œ í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
            return
        
        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
        test_queries = [
            "ë‹¹ë‡¨ë³‘ í™˜ìì˜ í˜ˆë‹¹ ê´€ë¦¬ ë°©ë²•",
            "ê³ í˜ˆì•• ì‘ê¸‰ìƒí™© ëŒ€ì²˜ë²•",
            "ì†Œì•„ ë°œì—´ ì‹œ ì²˜ì¹˜ ë°©ë²•"
        ]
        
        for query in test_queries:
            print(f"ğŸ” í…ŒìŠ¤íŠ¸: '{query}'")
            documents = searcher.search_medgemma(query)
            
            if documents:
                doc = documents[0]
                print(f"   âœ… ì‘ë‹µ ìƒì„± ì„±ê³µ")
                print(f"   ğŸ“„ ì‘ë‹µ ê¸¸ì´: {len(doc.page_content)}ì")
                print(f"   â­ í’ˆì§ˆ ì ìˆ˜: {doc.metadata.get('quality_score', 0)}/10")
                print(f"   ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: {doc.metadata.get('estimated_category', 'N/A')}")
            else:
                print("   âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
            print()
        
        # í†µê³„ ì¶œë ¥
        stats = searcher.get_stats()
        print("ğŸ“Š MedGemma í†µê³„:")
        print(f"   ì„±ê³µë¥ : {stats['performance']['success_rate']}%")
        print(f"   í‰ê·  ì‘ë‹µ ê¸¸ì´: {stats['performance']['average_response_length']}í† í°")
        print(f"   ì²˜ë¦¬ëœ ì§ˆë¬¸: {stats['performance']['queries_processed']}ê°œ")
        
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        searcher.cleanup()
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    test_medgemma_searcher()