# components/medgemma_searcher.py
from typing import List, Dict, Any, Optional
from langchain_core.documents import Document
from datetime import datetime
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import logging
import os
from huggingface_hub import login
from prompts import system_prompts

logger = logging.getLogger(__name__)

class MedGemmaSearcher:
    """MedGemma ì˜ë£Œ íŠ¹í™” LLM ê²€ìƒ‰ ë‹´ë‹¹ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = "google/gemma_2b_it", device: str = "auto"):
        """
        MedGemma ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  Gemma ëª¨ë¸ëª… (ì˜ë£Œ íŒŒì¸íŠœë‹ ë²„ì „ ê¶Œì¥)
            device: ì‹¤í–‰ ë””ë°”ì´ìŠ¤ ("auto", "cpu", "cuda")
        """

        self.model_name = model_name
        self.device = self._get_device(device)
        
        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        
        # ëª¨ë¸ ë¡œë“œ ì‹œë„
        self.model_loaded = True
        self._try_load_model()
        
        # ê²€ìƒ‰ í†µê³„
        self.search_stats = {
            "queries_processed": 0,
            "successful_generations": 0,
            "failed_generations": 0,
            "average_response_length": 0,
            "total_tokens_generated": 0
        }
    
    def _get_device(self, device: str) -> str:
        """ìµœì ì˜ ë””ë°”ì´ìŠ¤ ê²°ì •"""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            elif torch.backends.mps.is_available():  # Apple Silicon
                return "mps"
            else:
                return "cpu"
        return device
    
    def _try_load_model(self):
        """MedGemma ëª¨ë¸ ë¡œë“œ ì‹œë„"""
        print(f"ğŸ§  MedGemma ëª¨ë¸ ë¡œë”© ì¤‘... ({self.device})")
        
        try:
            # Hugging Face í† í° ë¡œë“œ
            import os
            from dotenv import load_dotenv
            load_dotenv()
            
            hf_token = os.getenv("HUGGINGFACE_HUB_TOKEN")
            if not hf_token:
                print("âš ï¸ Hugging Face í† í°ì´ ì—†ìŠµë‹ˆë‹¤")
            
            # ëª¨ë¸ ê²½ë¡œ í™•ì¸ (ë¡œì»¬ ìºì‹œ í™•ì¸)
            from huggingface_hub import snapshot_download
            print("ğŸ” ëª¨ë¸ ìºì‹œ í™•ì¸ ì¤‘...")
            try:
                model_path = snapshot_download(
                    repo_id=self.model_name,
                    token=hf_token,
                    local_files_only=True  # ë¡œì»¬ ìºì‹œë§Œ í™•ì¸
                )
                print(f"âœ… ë¡œì»¬ ìºì‹œì—ì„œ ëª¨ë¸ ë°œê²¬: {model_path}")
            except Exception:
                print("âš ï¸ ë¡œì»¬ ìºì‹œì— ëª¨ë¸ ì—†ìŒ, ë‹¤ìš´ë¡œë“œ í•„ìš”")
                model_path = self.model_name
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            print("ğŸ”„ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                token=hf_token
            )
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ëª¨ë¸ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ)
            print("ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                token=hf_token,
                offload_folder="medgemma_offload",
                offload_state_dict=True
            )

            # íŒŒì´í”„ë¼ì¸ ìƒì„± - attention_mask ëª…ì‹œì  ì²˜ë¦¬ ì„¤ì •
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device_map="auto", # device_mapì„ í•­ìƒ "auto"ë¡œ ì§€ì •
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                trust_remote_code=True
            )          

            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            test_prompt = "ì•ˆë…•í•˜ì„¸ìš”, ì˜ë£Œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."
            input_ids = self.tokenizer(test_prompt, return_tensors="pt").input_ids.to(self.model.device)
            with torch.no_grad():
                test_output = self.model.generate(input_ids, max_new_tokens=20)
            test_response = self.tokenizer.decode(test_output[0], skip_special_tokens=True)
            print(f"âœ… í…ŒìŠ¤íŠ¸ ì‘ë‹µ: '{test_response}'")
            
            self.model_loaded = True
            print(f"âœ… MedGemma ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({self.device})")
            
        except Exception as e:
            logger.error(f"MedGemma ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            print(f"âŒ MedGemma ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            self.model_loaded = False
    
    def search_medgemma(self, query: str, max_results: int = 3, max_length: int = 512) -> List[Document]:
        """MedGemmaë¥¼ ì‚¬ìš©í•œ ì˜ë£Œ ì§€ì‹ ê²€ìƒ‰"""
        print(f"==== [MEDGEMMA SEARCH: {query}] ====")
        
        self.search_stats["queries_processed"] += 1
        
        if not self.model_loaded:
            print("  âŒ MedGemma ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            return self._create_fallback_documents(query)
        
        try:
            # ëª¨ë¸ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            if torch.cuda.is_available():
                print(f"  ğŸ” CUDA ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated()/1024**2:.1f}MB / {torch.cuda.memory_reserved()/1024**2:.1f}MB")
            
            # ì˜ë£Œ íŠ¹í™” í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            medical_prompt = system_prompts.format("MEDGEMMA", query = query)
            
            # MedGemma ì¶”ë¡  ì‹¤í–‰
            response = self._generate_medical_response(medical_prompt, max_length)

            # ë””ë²„ê¹…ìš© ì „ì²´ ì‘ë‹µ ì¶œë ¥
            print(f"\n====== MEDGEMMA ì „ì²´ ì‘ë‹µ ì‹œì‘ ======")
            print(f"{response}")
            print(f"====== MEDGEMMA ì „ì²´ ì‘ë‹µ ë ======\n")
            
            print(f"  ğŸ” ì‘ë‹µ ê¸¸ì´: {len(response) if response else 0}ì")
            
            if response and len(response.strip()) > 10:  # ìµœì†Œ ê¸¸ì´ í™•ì¸
                # Document ê°ì²´ë¡œ ë³€í™˜
                document = self._convert_to_document(query, response)
                
                self.search_stats["successful_generations"] += 1
                self.search_stats["total_tokens_generated"] += len(response.split())
                
                print(f"  âœ… MedGemma ì‘ë‹µ ìƒì„± ì™„ë£Œ ({len(response)}ì)")
                return [document]
            else:
                print(f"  âŒ MedGemma ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ: '{response}'")
                # ì‹¤íŒ¨ ì´ìœ  ë¶„ì„
                if not response:
                    print("  ğŸ” ì‘ë‹µì´ Noneì„")
                elif len(response.strip()) <= 10:
                    print(f"  ğŸ” ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ: '{response}'")
                
                self.search_stats["failed_generations"] += 1
                return self._create_fallback_documents(query)
                
        except Exception as e:
            logger.error(f"MedGemma ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            print(f"  âŒ MedGemma ì˜¤ë¥˜: {str(e)}")
            self.search_stats["failed_generations"] += 1
            return self._create_fallback_documents(query)
        
    def _detect_medical_question_type(self, query: str) -> str:
        """ì˜ë£Œ ì§ˆë¬¸ ìœ í˜• ê°ì§€"""
        query_lower = query.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        if any(word in query_lower for word in ["ì‘ê¸‰", "ê¸‰ì„±", "ì‹¬ì •ì§€", "ì‡¼í¬", "ì¶œí˜ˆ"]):
            return "emergency"
        elif any(word in query_lower for word in ["ì§„ë‹¨", "ê²€ì‚¬", "ì¦ìƒ", "ì›ì¸"]):
            return "diagnosis"
        elif any(word in query_lower for word in ["ì¹˜ë£Œ", "ì²˜ì¹˜", "ê´€ë¦¬", "ìš”ë²•"]):
            return "treatment"
        elif any(word in query_lower for word in ["ì•½ë¬¼", "ì•½", "ì²˜ë°©", "ë¶€ì‘ìš©"]):
            return "medication"
        elif any(word in query_lower for word in ["ìˆ˜ìˆ ", "ì‹œìˆ ", "ì ˆì°¨", "í”„ë¡œí† ì½œ"]):
            return "procedure"
        else:
            return "general"
    
    def _generate_medical_response(self, prompt: str, max_length: int) -> Optional[str]:
        """MedGemmaë¥¼ ì‚¬ìš©í•œ ì˜ë£Œ ì‘ë‹µ ìƒì„±"""
        
        try:
            print(f"    ğŸ¤– MedGemma ì¶”ë¡  ì‹œì‘... (max_tokens: {max_length})")
            print(f"    ğŸ“‹ í”„ë¡¬í”„íŠ¸: '{prompt[:200]}...'")
            
            # í† í¬ë‚˜ì´ì € ì„¤ì • í™•ì¸
            inputs = self.tokenizer(prompt, return_tensors="pt", padding=True).to(self.device)
            
            # ì§ì ‘ ëª¨ë¸ generate ë©”ì„œë“œ ì‚¬ìš© (íŒŒì´í”„ë¼ì¸ ëŒ€ì‹ )
            with torch.no_grad():
                output_ids = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=max_length,
                    min_new_tokens=100,  # ìµœì†Œ í† í° ìˆ˜ ì„¤ì •
                    do_sample=True,
                    temperature=0.8,  # ë†’ì€ ì˜¨ë„ ì„¤ì •
                    top_p=0.9,
                    repetition_penalty=1.2,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            # ë””ì½”ë”©
            generated_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ì œê±°
            if generated_text.startswith(prompt):
                generated_text = generated_text[len(prompt):].strip()
            
            print(f"    ğŸ“ ì›ë³¸ ì‘ë‹µ ê¸¸ì´: {len(generated_text)}ì")
            print(f"    ğŸ“ ì›ë³¸ ì‘ë‹µ ì‹œì‘ ë¶€ë¶„: '{generated_text[:100]}...'")
            
            if len(generated_text) < 10:  # ì‘ë‹µì´ ë„ˆë¬´ ì§§ìœ¼ë©´
                print(f"    âš ï¸ ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ, ì¬ì‹œë„...")
                # ì¬ì‹œë„ ë¡œì§ (ì˜¨ë„ ë³€ê²½)
                with torch.no_grad():
                    output_ids = self.model.generate(
                        inputs.input_ids,
                        max_new_tokens=max_length,
                        min_new_tokens=150,
                        do_sample=True,
                        temperature=1.0,  # ë” ë†’ì€ ì˜¨ë„
                        top_p=0.95,
                        repetition_penalty=1.3,
                    )
                generated_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
                if generated_text.startswith(prompt):
                    generated_text = generated_text[len(prompt):].strip()
            
            # ì‘ë‹µ ì •ë¦¬
            cleaned_response = self._clean_medical_response(generated_text)
            return cleaned_response
                
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            print(f"    âŒ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _clean_medical_response(self, response: str) -> str:
        """ìƒì„±ëœ ì˜ë£Œ ì‘ë‹µ ì •ë¦¬"""
        
        # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
        clean_response = response.strip()
        
        # ë°˜ë³µ íŒ¨í„´ ì œê±°
        lines = clean_response.split('\n')
        unique_lines = []
        seen_lines = set()
        
        for line in lines:
            line_clean = line.strip()
            if line_clean and line_clean not in seen_lines:
                unique_lines.append(line)
                seen_lines.add(line_clean)
        
        clean_response = '\n'.join(unique_lines)
        
        # ì˜ë£Œ ì •ë³´ ê²€ì¦ ë§ˆí¬ ì¶”ê°€
        if len(clean_response) > 50:
            clean_response += "\n\nâš ï¸ ì´ ì •ë³´ëŠ” AIê°€ ìƒì„±í•œ ê²ƒìœ¼ë¡œ, ì‹¤ì œ ì§„ë£Œ ì‹œì—ëŠ” ë°˜ë“œì‹œ ì˜ë£Œì§„ê³¼ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        
        return clean_response
    
    def _convert_to_document(self, query: str, response: str) -> Document:
        """MedGemma ì‘ë‹µì„ Document ê°ì²´ë¡œ ë³€í™˜"""
        
        # ì‘ë‹µ í’ˆì§ˆ í‰ê°€
        quality_score = self._assess_response_quality(response)
        
        # ì˜ë£Œ ì¹´í…Œê³ ë¦¬ ì¶”ì •
        estimated_category = self._estimate_medical_category(query, response)
        
        # Document ìƒì„±
        content = f"""MedGemma ì˜ë£Œ ì§€ì‹ ì‘ë‹µ:

ì§ˆë¬¸: {query}

ë‹µë³€:
{response}

ìƒì„± ì •ë³´:
- ëª¨ë¸: {self.model_name}
- ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- í’ˆì§ˆ ì ìˆ˜: {quality_score}/10
"""

        metadata = {
            "source": f"medgemma_{self.model_name}",
            "source_type": "medgemma",
            "model_name": self.model_name,
            "query": query,
            "generated_at": datetime.now().isoformat(),
            "device": self.device,
            "quality_score": quality_score,
            "estimated_category": estimated_category,
            "response_length": len(response),
            "reliability": "high",  # MedGemmaëŠ” ì˜ë£Œ íŠ¹í™” ëª¨ë¸
            "confidence": "high" if quality_score >= 7 else "medium"
        }
        
        return Document(page_content=content, metadata=metadata)
    
    def _assess_response_quality(self, response: str) -> float:
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€ (1-10ì )"""
        score = 5.0  # ê¸°ë³¸ ì ìˆ˜
        
        # ê¸¸ì´ í‰ê°€
        if 100 <= len(response) <= 1000:
            score += 1.0
        elif len(response) > 1000:
            score += 0.5
        
        # ì˜ë£Œ ìš©ì–´ í¬í•¨ ì—¬ë¶€
        medical_terms = ["ì¹˜ë£Œ", "ì§„ë‹¨", "ì¦ìƒ", "ì•½ë¬¼", "ì²˜ë°©", "í™˜ì", "ì˜ë£Œì§„", "ë³‘ì›"]
        term_count = sum(1 for term in medical_terms if term in response)
        score += min(2.0, term_count * 0.3)
        
        # êµ¬ì¡°í™”ëœ ì •ë³´ ì—¬ë¶€ (ë²ˆí˜¸, ë‹¨ê³„ ë“±)
        if any(pattern in response for pattern in ["1.", "2.", "ì²«ì§¸", "ë‘˜ì§¸", "ë‹¨ê³„"]):
            score += 1.0
        
        # ì•ˆì „ ì •ë³´ í¬í•¨ ì—¬ë¶€
        if any(word in response for word in ["ì£¼ì˜", "ê²½ê³ ", "ë¶€ì‘ìš©", "ê¸ˆê¸°"]):
            score += 1.0
        
        return min(10.0, score)
    
    def _estimate_medical_category(self, query: str, response: str) -> str:
        """ì˜ë£Œ ì¹´í…Œê³ ë¦¬ ì¶”ì •"""
        combined_text = f"{query} {response}".lower()
        
        category_keywords = {
            "ì‘ê¸‰ì²˜ì¹˜": ["ì‘ê¸‰", "ê¸‰ì„±", "ì‹¬ì •ì§€", "ì‘ê¸‰ì²˜ì¹˜"],
            "ë‚´ê³¼": ["ë‹¹ë‡¨", "ê³ í˜ˆì••", "ë‚´ê³¼", "ë§Œì„±ì§ˆí™˜"],
            "ì™¸ê³¼": ["ìˆ˜ìˆ ", "ì™¸ê³¼", "ì ˆê°œ", "ë´‰í•©"],
            "ì•½ë¬¼ì •ë³´": ["ì•½ë¬¼", "ì²˜ë°©", "ë¶€ì‘ìš©", "ìš©ë²•"],
            "ì§„ë‹¨ê²€ì‚¬": ["ì§„ë‹¨", "ê²€ì‚¬", "ì˜ìƒ", "í˜ˆì•¡"],
            "ê°ì—¼ê´€ë¦¬": ["ê°ì—¼", "í•­ìƒì œ", "ë°”ì´ëŸ¬ìŠ¤", "ì„¸ê· "]
        }
        
        max_matches = 0
        best_category = "ì¼ë°˜ì˜í•™"
        
        for category, keywords in category_keywords.items():
            matches = sum(1 for keyword in keywords if keyword in combined_text)
            if matches > max_matches:
                max_matches = matches
                best_category = category
        
        return best_category
    
    def _create_fallback_documents(self, query: str) -> List[Document]:
        """MedGemma ì‹¤íŒ¨ ì‹œ í´ë°± ë¬¸ì„œ ìƒì„±"""
        fallback_content = f"""
MedGemma ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ ì˜ë£Œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì§ˆë¬¸: {query}

ì¼ë°˜ì ì¸ ì˜ë£Œ ê°€ì´ë“œë¼ì¸:
1. ì •í™•í•œ ì§„ë‹¨ì„ ìœ„í•´ì„œëŠ” ì˜ë£Œì§„ê³¼ ì§ì ‘ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤
2. ì‘ê¸‰ìƒí™© ì‹œì—ëŠ” ì¦‰ì‹œ 119ì— ì‹ ê³ í•˜ì„¸ìš”
3. ì•½ë¬¼ ë³µìš© ì „ì—ëŠ” ë°˜ë“œì‹œ ì „ë¬¸ì˜ì™€ ìƒì˜í•˜ì„¸ìš”
4. ì¦ìƒì´ ì§€ì†ë˜ê±°ë‚˜ ì•…í™”ë˜ë©´ ë³‘ì› ë°©ë¬¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤

âš ï¸ ì´ëŠ” MedGemma ëª¨ë¸ ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ ì•ˆë‚´ì‚¬í•­ì…ë‹ˆë‹¤.
ì •í™•í•œ ì˜ë£Œ ì •ë³´ëŠ” ì˜ë£Œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.
"""
        
        return [Document(
            page_content=fallback_content,
            metadata={
                "source": "medgemma_fallback",
                "source_type": "medgemma",
                "query": query,
                "fallback_reason": "model_unavailable",
                "reliability": "low",
                "confidence": "low",
                "generated_at": datetime.now().isoformat()
            }
        )]
    
    def get_stats(self) -> Dict[str, Any]:
        """MedGemma ê²€ìƒ‰ê¸° í†µê³„"""
        success_rate = 0
        if self.search_stats["queries_processed"] > 0:
            success_rate = self.search_stats["successful_generations"] / self.search_stats["queries_processed"]
        
        avg_length = 0
        if self.search_stats["successful_generations"] > 0:
            avg_length = self.search_stats["total_tokens_generated"] / self.search_stats["successful_generations"]
        
        return {
            "searcher_type": "MedGemmaSearcher",
            "model_info": {
                "model_name": self.model_name,
                "device": self.device,
                "model_loaded": self.model_loaded
            },
            "performance": {
                "queries_processed": self.search_stats["queries_processed"],
                "successful_generations": self.search_stats["successful_generations"],
                "failed_generations": self.search_stats["failed_generations"],
                "success_rate": round(success_rate * 100, 2),
                "average_response_length": round(avg_length, 1)
            },
            "resource_usage": {
                "total_tokens_generated": self.search_stats["total_tokens_generated"],
                "estimated_cost_usd": 0.0  # ë¡œì»¬ ì‹¤í–‰ì´ë¯€ë¡œ ë¹„ìš© ì—†ìŒ
            }
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.model is not None:
                del self.model
            if self.tokenizer is not None:
                del self.tokenizer
            if self.pipeline is not None:
                del self.pipeline
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            print("ğŸ—‘ï¸ MedGemma ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
def test_medgemma_searcher():
    """MedGemma ê²€ìƒ‰ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª MedGemma ê²€ìƒ‰ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    try:
        # ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        searcher = MedGemmaSearcher()
        
        if not searcher.model_loaded:
            print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ë¡œ í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
            return
        
        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
        test_queries = [
            "ë‹¹ë‡¨ë³‘ í™˜ìì˜ í˜ˆë‹¹ ê´€ë¦¬ ë°©ë²•",
            "ê³ í˜ˆì•• ì‘ê¸‰ìƒí™© ëŒ€ì²˜ë²•",
            "ì†Œì•„ ë°œì—´ ì‹œ ì²˜ì¹˜ ë°©ë²•"
        ]
        
        for query in test_queries:
            print(f"ğŸ” í…ŒìŠ¤íŠ¸: '{query}'")
            documents = searcher.search_medgemma(query)
            
            if documents:
                doc = documents[0]
                print(f"   âœ… ì‘ë‹µ ìƒì„± ì„±ê³µ")
                print(f"   ğŸ“„ ì‘ë‹µ ê¸¸ì´: {len(doc.page_content)}ì")
                print(f"   â­ í’ˆì§ˆ ì ìˆ˜: {doc.metadata.get('quality_score', 0)}/10")
                print(f"   ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: {doc.metadata.get('estimated_category', 'N/A')}")
            else:
                print("   âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
            print()
        
        # í†µê³„ ì¶œë ¥
        stats = searcher.get_stats()
        print("ğŸ“Š MedGemma í†µê³„:")
        print(f"   ì„±ê³µë¥ : {stats['performance']['success_rate']}%")
        print(f"   í‰ê·  ì‘ë‹µ ê¸¸ì´: {stats['performance']['average_response_length']}í† í°")
        print(f"   ì²˜ë¦¬ëœ ì§ˆë¬¸: {stats['performance']['queries_processed']}ê°œ")
        
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        searcher.cleanup()
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    test_medgemma_searcher()