# components/memory_manager.py
from typing import List, Dict, Any, Optional
from datetime import datetime
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from prompts import system_prompts

class MemoryManager:
    """íš¨ìœ¨ì ì¸ ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.max_history = 20
        self.summary_cache = None
        self._setup_summary_chain()
        print("ğŸ’­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_summary_chain(self):
        """ëŒ€í™” ìš”ì•½ ì²´ì¸ ì„¤ì •"""
        self.summary_prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompts.format("MEMORY", language="í•œêµ­ì–´")),
            
            ("human", """Summarize this medical conversation:
            
            {conversation_history}
            
            Provide a concise summary:""")
        ])
        
        self.summary_chain = self.summary_prompt | self.llm
    
    def manage_conversation_memory(self, conversation_history: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬ - 20ê°œ ì´ˆê³¼ì‹œì—ë§Œ ìš”ì•½"""
        
        if len(conversation_history) <= self.max_history:
            # 20ê°œ ì´í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return conversation_history
        
        print(f"ğŸ“ ëŒ€í™” ì´ë ¥ ê´€ë¦¬: {len(conversation_history)}ê°œ â†’ ìš”ì•½ + ìµœê·¼ 10ê°œ")
        
        # 20ê°œ ì´ˆê³¼ì‹œ: ìš”ì•½ ìƒì„± (1íšŒë§Œ)
        if self.summary_cache is None:
            old_conversations = conversation_history[:-10]  # ì˜¤ë˜ëœ ëŒ€í™”ë“¤
            self.summary_cache = self._create_conversation_summary(old_conversations)
        
        # ìš”ì•½ ë©”ì‹œì§€ ìƒì„±
        summary_message = {
            "role": "system",
            "content": f"ì´ì „ ëŒ€í™” ìš”ì•½: {self.summary_cache}",
            "timestamp": datetime.now().isoformat(),
            "message_type": "conversation_summary"
        }
        
        # ìš”ì•½ + ìµœê·¼ 10ê°œ ëŒ€í™” ë°˜í™˜
        recent_conversations = conversation_history[-10:]
        managed_history = [summary_message] + recent_conversations
        
        print(f"  âœ… ìš”ì•½ ì™„ë£Œ: 1ê°œ ìš”ì•½ + {len(recent_conversations)}ê°œ ìµœê·¼ ëŒ€í™”")
        return managed_history
    
    def _create_conversation_summary(self, conversations: List[Dict[str, Any]]) -> str:
        """ëŒ€í™” ìš”ì•½ ìƒì„± (1íšŒë§Œ ì‹¤í–‰)"""
        print("  ğŸ”„ ëŒ€í™” ìš”ì•½ ìƒì„± ì¤‘...")
        
        try:
            # ëŒ€í™” ì´ë ¥ í¬ë§·íŒ…
            formatted_history = self._format_conversations(conversations)
            
            # ìš”ì•½ ìƒì„±
            response = self.summary_chain.invoke({
                "conversation_history": formatted_history
            })
            
            summary = response.content if hasattr(response, 'content') else str(response)
            print(f"  âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ: {len(conversations)}ê°œ ëŒ€í™” â†’ {len(summary)}ì")
            
            return summary
            
        except Exception as e:
            print(f"  âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return self._create_fallback_summary(conversations)
    
    def _format_conversations(self, conversations: List[Dict[str, Any]]) -> str:
        """ëŒ€í™” ì´ë ¥ì„ ìš”ì•½ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        formatted_parts = []
        
        for i, conv in enumerate(conversations):
            role = "ğŸ‘¤ ì‚¬ìš©ì" if conv.get("role") == "user" else "ğŸ¤– AI"
            content = conv.get("content", "")[:150]  # 150ì ì œí•œ
            timestamp = conv.get("timestamp", "")[:16]  # ë‚ ì§œì‹œê°„ ë‹¨ì¶•
            
            formatted_parts.append(f"[{timestamp}] {role}: {content}")
        
        return "\n".join(formatted_parts)
    
    def _create_fallback_summary(self, conversations: List[Dict[str, Any]]) -> str:
        """ìš”ì•½ ìƒì„± ì‹¤íŒ¨ ì‹œ í´ë°± ìš”ì•½"""
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ ìš”ì•½
        medical_keywords = []
        user_questions = 0
        
        for conv in conversations:
            if conv.get("role") == "user":
                user_questions += 1
                content = conv.get("content", "").lower()
                
                # ì˜ë£Œ í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = ["ë‹¹ë‡¨", "ê³ í˜ˆì••", "ì‘ê¸‰", "ì•½ë¬¼", "ì¹˜ë£Œ", "ì¦ìƒ", "ì§„ë‹¨", "ìˆ˜ìˆ "]
                for keyword in keywords:
                    if keyword in content and keyword not in medical_keywords:
                        medical_keywords.append(keyword)
        
        summary = f"ì´ {user_questions}ê°œì˜ ì˜ë£Œ ì§ˆë¬¸ì´ ìˆì—ˆìŠµë‹ˆë‹¤."
        
        if medical_keywords:
            summary += f" ì£¼ìš” ì£¼ì œ: {', '.join(medical_keywords[:5])}"
        
        return summary
        
    def enhance_question_with_context(self, conversation_history: List[Dict[str, Any]], current_question: str) -> str:
        """ì´ì „ ëŒ€í™”ê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ë§¥ë½ì„ í¬í•¨í•œ ì§ˆë¬¸ìœ¼ë¡œ ì¬ìƒì„±"""
        
        if not conversation_history or len(conversation_history) < 2:
            return current_question  # ì²« ì§ˆë¬¸ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
        
        print(f"ğŸ”— ì´ì „ ëŒ€í™” ë§¥ë½ ë¶„ì„ ì¤‘... (ì´ {len(conversation_history)}ê°œ ëŒ€í™”)")
        
        # ìµœê·¼ ëŒ€í™” ë‚´ìš© ì¶”ì¶œ (ìµœëŒ€ 5ê°œ í„´)
        recent_context = self._extract_recent_context(conversation_history[-10:])
        
        # LLMì„ ì‚¬ìš©í•´ ë§¥ë½ì´ í¬í•¨ëœ ì§ˆë¬¸ ì¬ìƒì„±
        enhanced_question = self._generate_context_aware_question(recent_context, current_question)
        
        print(f"  ğŸ“ ì›ë˜ ì§ˆë¬¸: {current_question}")
        print(f"  âœ¨ ë§¥ë½ í¬í•¨ ì§ˆë¬¸: {enhanced_question[:100]}...")
        
        return enhanced_question

    def _extract_recent_context(self, recent_history: List[Dict[str, Any]]) -> str:
        """ìµœê·¼ ëŒ€í™”ì—ì„œ ë§¥ë½ ì •ë³´ ì¶”ì¶œ"""
        context_parts = []
        
        for i, turn in enumerate(recent_history):
            role = turn.get("role", "")
            content = turn.get("content", "")
            
            if role == "user":
                context_parts.append(f"ì‚¬ìš©ì ì§ˆë¬¸ {i+1}: {content}")
            elif role == "assistant":
                # ë‹µë³€ì€ í•µì‹¬ ë‚´ìš©ë§Œ ìš”ì•½ (200ì ì œí•œ)
                content_summary = content[:200] + "..." if len(content) > 200 else content
                context_parts.append(f"AI ë‹µë³€ {i+1}: {content_summary}")
            elif role == "system" and turn.get("message_type") == "conversation_summary":
                context_parts.append(f"ì´ì „ ëŒ€í™” ìš”ì•½: {content}")
        
        return "\n".join(context_parts[-6:])  # ìµœê·¼ 6ê°œ í„´ë§Œ

    def _generate_context_aware_question(self, context: str, current_question: str) -> str:
        """LLMì„ ì‚¬ìš©í•´ ë§¥ë½ì´ í¬í•¨ëœ ì§ˆë¬¸ ìƒì„±"""
        try:
            from langchain_core.prompts import ChatPromptTemplate
            
            context_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ì˜ë£Œ ëŒ€í™”ì—ì„œ ë§¥ë½ì„ ì´í•´í•˜ì—¬ ì§ˆë¬¸ì„ ê°œì„ í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.

    ì§€ì¹¨:
    1. ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ì˜ë£Œ ì£¼ì œ, ì¦ìƒ, ì¹˜ë£Œë²• ë“±ì„ í˜„ì¬ ì§ˆë¬¸ì— ì—°ê²°
    2. ì• ë§¤í•œ ì§€ì‹œì–´("ê·¸ê²ƒ", "ì´ê²ƒ", "ê·¸ ë°©ë²•" ë“±)ë¥¼ êµ¬ì²´ì ì¸ ë‚´ìš©ìœ¼ë¡œ êµì²´
    3. ì˜ë£Œì  ë§¥ë½ì„ ëª…í™•íˆ í•˜ì—¬ ë” ì •í™•í•œ ë‹µë³€ì´ ê°€ëŠ¥í•˜ë„ë¡ ê°œì„ 
    4. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±
    5. ì›ë˜ ì§ˆë¬¸ì˜ ì˜ë„ëŠ” ìœ ì§€í•˜ë˜ ë§¥ë½ ì •ë³´ ì¶”ê°€

    ì˜ˆì‹œ:
    - ì›ë˜: "ë¶€ì‘ìš©ì€ ë­ê°€ ìˆì–´?"
    - ê°œì„ : "ì•ì„œ ì–¸ê¸‰í•œ ë©”íŠ¸í¬ë¯¼ì˜ ë¶€ì‘ìš©ì€ ë­ê°€ ìˆì–´?"

    - ì›ë˜: "ë‹¤ë¥¸ ë°©ë²•ì€?"  
    - ê°œì„ : "ë‹¹ë‡¨ë³‘ ê´€ë¦¬ì—ì„œ ì•½ë¬¼ ì¹˜ë£Œ ì™¸ì˜ ë‹¤ë¥¸ ë°©ë²•ì€?"
    """),
                ("human", """ì´ì „ ëŒ€í™” ë§¥ë½:
    {context}

    í˜„ì¬ ì§ˆë¬¸: {current_question}

    ìœ„ ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”:""")
            ])
            
            enhanced_question = self.llm.invoke(
                context_prompt.format(context=context, current_question=current_question)
            ).content
            
            return enhanced_question.strip()
            
        except Exception as e:
            print(f"  âŒ ë§¥ë½ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # í´ë°±: ê°„ë‹¨í•œ ë§¥ë½ ì¶”ê°€
            return self._simple_context_enhancement(context, current_question)

    def _simple_context_enhancement(self, context: str, current_question: str) -> str:
        """LLM ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ ë§¥ë½ ì¶”ê°€"""
        # ìµœê·¼ ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ì¶œ
        recent_topics = []
        for line in context.split('\n'):
            if 'ì‚¬ìš©ì ì§ˆë¬¸' in line:
                topic = line.split(': ', 1)[-1]
                recent_topics.append(topic)
        
        if recent_topics:
            latest_topic = recent_topics[-1][:50]
            return f"ì´ì „ ëŒ€í™”ì—ì„œ '{latest_topic}'ì— ëŒ€í•´ ë…¼ì˜í–ˆëŠ”ë°, {current_question}"
        
        return f"ì´ì „ ëŒ€í™” ë§¥ë½ì—ì„œ ì´ì–´ì„œ, {current_question}"

    def reset_summary_cache(self):
        """ìš”ì•½ ìºì‹œ ì´ˆê¸°í™” (í•„ìš”ì‹œ ì‚¬ìš©)"""
        self.summary_cache = None
        print("ğŸ”„ ìš”ì•½ ìºì‹œ ì´ˆê¸°í™”ë¨")