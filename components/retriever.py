# components/retriever.py (ì™„ì „í•œ ë¦¬íŒ©í† ë§ëœ ë²„ì „)
"""
ë¬¸ì„œ ê²€ìƒ‰ ì „ìš© í´ë˜ìŠ¤ - ì„ë² ë”© ìƒì„± ë° ìœ ì‚¬ë„ ê²€ìƒ‰ì— ì§‘ì¤‘
ë¬¸ì„œ ë¡œë”©ì€ DocumentLoaderì—ê²Œ ìœ„ì„
"""

import os
import json
import openai
import numpy as np
import pickle
import hashlib
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime, timedelta
from langchain_core.documents import Document
from components.parallel_searcher import ParallelSearcher
from components.document_loader import DocumentLoader
from config import Config
import logging

logger = logging.getLogger(__name__)

class Retriever:
    """ë¦¬íŒ©í† ë§ëœ ê²€ìƒ‰ ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ê²€ìƒ‰ê¸° ì´ˆê¸°í™” - ê²€ìƒ‰ ê¸°ëŠ¥ì—ë§Œ ì§‘ì¤‘"""
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        
        self.client = openai.OpenAI(api_key=api_key)
        self.model_name = "text-embedding-3-large"
        
        # ë¬¸ì„œ ë¡œë” (ìœ„ì„)
        self.document_loader = DocumentLoader()
        
        # ìºì‹œ ì„¤ì •
        self.cache_dir = Path("./embedding_cache")
        self.cache_dir.mkdir(exist_ok=True)
        self.cache_enabled = True
        
        # ê²€ìƒ‰ìš© ë°ì´í„°
        self.medical_documents = []
        self.document_embeddings = []
        self.embedding_index = {}
        
        # ê²€ìƒ‰ í†µê³„
        self.search_stats = {
            "api_calls": 0,
            "cache_hits": 0,
            "total_tokens": 0,
            "searches_performed": 0,
            "average_response_time": 0.0
        }
        
        from components.pubMed_searcher import PubMedSearcher
        self.pubmed_searcher = PubMedSearcher(
            email="medical.chatbot@example.com",
            api_key=None
        )
        
        # ìºì‹œ íŒŒì¼ ê²½ë¡œ
        self.embeddings_file = Path("./embeddings_cache.pkl")
        self.documents_file = Path("./documents_cache.pkl")
        
        print("ğŸ” ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
        self._load_cached_embeddings()
        print(f"âœ… ê²€ìƒ‰ê¸° ì¤€ë¹„ ì™„ë£Œ! í˜„ì¬ ë¬¸ì„œ: {len(self.medical_documents)}ê°œ")
    
    def retrieve_documents(self, question: str, k: int = 5) -> List[Document]:
        """ì˜ë£Œ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        import time
        start_time = time.time()
        
        print(f"==== [SEARCH: {question[:50]}...] ====")
        
        try:
            # 1. ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
            question_embedding = self._get_embedding(question)
            
            # 2. ì˜ë£Œ í‚¤ì›Œë“œ ê¸°ë°˜ ì‚¬ì „ í•„í„°ë§
            candidate_indices = self._get_candidate_documents(question)
            
            # 3. ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            indices_to_check = candidate_indices if candidate_indices else range(len(self.medical_documents))
            
            for i in indices_to_check:
                if i < len(self.document_embeddings):
                    similarity = self._cosine_similarity(question_embedding, self.document_embeddings[i])
                    similarities.append((similarity, i))
            
            # 4. ìœ ì‚¬ë„ìˆœ ì •ë ¬ ë° ì„ê³„ê°’ ì ìš©
            similarities.sort(reverse=True)
            
            # 5. ìƒìœ„ ë¬¸ì„œ ì„ íƒ
            top_documents = []
            threshold = getattr(Config, 'SIMILARITY_THRESHOLD', 0.3)
            
            for similarity, idx in similarities[:k*2]:
                if similarity >= threshold:
                    doc = self.medical_documents[idx].copy()
                    doc.metadata["similarity_score"] = round(similarity, 4)
                    doc.metadata["search_rank"] = len(top_documents) + 1
                    doc.metadata["search_question"] = question
                    top_documents.append(doc)
            
            # 6. ì˜ë£Œ ê´€ë ¨ì„± ì¬ê²€ì¦
            filtered_docs = self._medical_relevance_filter(top_documents, question)[:k]
            
            # ê²€ìƒ‰ í†µê³„ ì—…ë°ì´íŠ¸
            response_time = time.time() - start_time
            self._update_search_stats(response_time)
            
            print(f"  ğŸ“Š ê²€ìƒ‰ ê²°ê³¼:")
            print(f"    í›„ë³´: {len(indices_to_check)}ê°œ â†’ ìœ ì‚¬: {len([s for s, _ in similarities if s >= threshold])}ê°œ â†’ ìµœì¢…: {len(filtered_docs)}ê°œ")
            print(f"    ì‘ë‹µì‹œê°„: {response_time:.2f}ì´ˆ")
            if filtered_docs:
                print(f"    ìµœê³  ìœ ì‚¬ë„: {filtered_docs[0].metadata.get('similarity_score', 0):.3f}")
            
            return filtered_docs
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return self._get_emergency_fallback_docs(question)
    
    def load_documents_from_directory(self, directory_path: str) -> int:
        """ë¬¸ì„œ ë¡œë”© (DocumentLoaderì—ê²Œ ìœ„ì„)"""
        print(f"ğŸ“š ë¬¸ì„œ ë¡œë”© ìš”ì²­: {directory_path}")
        
        # ê¸°ì¡´ ë¬¸ì„œ ìˆ˜
        initial_count = len(self.medical_documents)
        
        # DocumentLoaderë¥¼ í†µí•´ ë¬¸ì„œ ë¡œë”©
        new_documents = self.document_loader.load_documents_from_directory(directory_path)
        
        if not new_documents:
            print("ğŸ“­ ìƒˆë¡œ ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
            return 0
        
        # ì¤‘ë³µ ë¬¸ì„œ ì²´í¬
        existing_sources = {doc.metadata.get("source", "") for doc in self.medical_documents}
        unique_documents = []
        
        for doc in new_documents:
            source = doc.metadata.get("source", "")
            if source not in existing_sources:
                unique_documents.append(doc)
            else:
                print(f"  â­ï¸ ì¤‘ë³µ ê±´ë„ˆëœ€: {Path(source).name}")
        
        if not unique_documents:
            print("ğŸ“­ ìƒˆë¡œìš´ ê³ ìœ  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë‘ ì¤‘ë³µ)")
            return 0
        
        # ìƒˆ ë¬¸ì„œë“¤ ì„ë² ë”© ìƒì„±
        print(f"ğŸ§  {len(unique_documents)}ê°œ ì‹ ê·œ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        texts_to_embed = [doc.page_content for doc in unique_documents]
        new_embeddings = self._batch_generate_embeddings(texts_to_embed)
        
        # ê¸°ì¡´ ë°ì´í„°ì— ì¶”ê°€
        self.medical_documents.extend(unique_documents)
        self.document_embeddings.extend(new_embeddings)
        
        # ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
        self._update_document_index()
        
        # ìºì‹œ ì €ì¥
        self._save_embeddings_cache()
        
        loaded_count = len(unique_documents)
        print(f"âœ… ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: {loaded_count}ê°œ ì‹ ê·œ, ì´ {len(self.medical_documents)}ê°œ")
        
        return loaded_count
    
    def _get_embedding(self, text: str, use_cache: bool = True) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ OpenAI ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        
        # ìºì‹œ í™•ì¸
        if use_cache and self.cache_enabled:
            cached = self._get_cached_embedding(text)
            if cached is not None:
                self.search_stats["cache_hits"] += 1
                return cached
        
        try:
            response = self.client.embeddings.create(
                model=self.model_name,
                input=text
            )
            
            embedding = response.data[0].embedding
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.search_stats["api_calls"] += 1
            self.search_stats["total_tokens"] += response.usage.total_tokens
            
            # ìºì‹œ ì €ì¥
            if use_cache and self.cache_enabled:
                self._save_cached_embedding(text, embedding)
            
            return embedding
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return [0.0] * 3072
    
    def _batch_generate_embeddings(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°°ì¹˜ë¡œ ìƒì„±"""
        all_embeddings = []
        
        print(f"  ğŸ”„ {len(texts)}ê°œ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            batch_start = i + 1
            batch_end = min(i + batch_size, len(texts))
            
            print(f"    ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬: {batch_start}-{batch_end}/{len(texts)}")
            
            try:
                response = self.client.embeddings.create(
                    model=self.model_name,
                    input=batch
                )
                
                batch_embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(batch_embeddings)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.search_stats["api_calls"] += 1
                self.search_stats["total_tokens"] += response.usage.total_tokens
                
            except Exception as e:
                logger.error(f"ë°°ì¹˜ ì„ë² ë”© ì‹¤íŒ¨: {str(e)}")
                fallback_embeddings = [[0.0] * 3072] * len(batch)
                all_embeddings.extend(fallback_embeddings)
        
        return all_embeddings
    
    def _get_cached_embedding(self, text: str) -> Optional[List[float]]:
        """ìºì‹œëœ ì„ë² ë”© ì¡°íšŒ"""
        text_hash = hashlib.md5(text.encode()).hexdigest()
        cache_file = self.cache_dir / f"{text_hash}.pkl"
        
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    cached_data = pickle.load(f)
                
                if datetime.now() - cached_data['timestamp'] < timedelta(days=7):
                    return cached_data['embedding']
            except:
                pass
        
        return None
    
    def _save_cached_embedding(self, text: str, embedding: List[float]):
        """ì„ë² ë”©ì„ ìºì‹œì— ì €ì¥"""
        text_hash = hashlib.md5(text.encode()).hexdigest()
        cache_file = self.cache_dir / f"{text_hash}.pkl"
        
        try:
            cache_data = {
                'embedding': embedding,
                'timestamp': datetime.now(),
                'model': self.model_name,
                'text_preview': text[:100]
            }
            
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
        except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def _load_cached_embeddings(self):
        """ì €ì¥ëœ ì„ë² ë”©ê³¼ ë¬¸ì„œ ë¡œë“œ"""
        try:
            if self.embeddings_file.exists() and self.documents_file.exists():
                print("ğŸ’¾ ê¸°ì¡´ ì„ë² ë”© ë°ì´í„° ë¡œë”© ì¤‘...")
                
                with open(self.documents_file, 'rb') as f:
                    self.medical_documents = pickle.load(f)
                
                with open(self.embeddings_file, 'rb') as f:
                    self.document_embeddings = pickle.load(f)
                
                self._update_document_index()
                
                print(f"âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.medical_documents)}ê°œ ë¬¸ì„œ")
                return True
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        
        return False

    def _save_embeddings_cache(self):
        """ì„ë² ë”©ê³¼ ë¬¸ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            print("ğŸ’¾ ì„ë² ë”© ë°ì´í„° ì €ì¥ ì¤‘...")
            
            with open(self.documents_file, 'wb') as f:
                pickle.dump(self.medical_documents, f)
            
            with open(self.embeddings_file, 'wb') as f:
                pickle.dump(self.document_embeddings, f)
            
            print("âœ… ì„ë² ë”© ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def _get_candidate_documents(self, question: str) -> Optional[List[int]]:
        """ì§ˆë¬¸ì—ì„œ ì˜ë£Œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ í›„ë³´ ë¬¸ì„œ í•„í„°ë§"""
        
        medical_keyword_map = {
            "ë‚™ìƒ": ["ë‚™ìƒ", "ì™¸ìƒ", "ê³¨ì ˆ"],
            "ë‹¹ë‡¨": ["ë‹¹ë‡¨ë³‘", "í˜ˆë‹¹", "ì¸ìŠë¦°"],
            "ê³ í˜ˆì••": ["ê³ í˜ˆì••", "í˜ˆì••", "ì‹¬í˜ˆê´€"],
            "ì‹¬ì •ì§€": ["ì‹¬ì •ì§€", "CPR", "ì‘ê¸‰ì²˜ì¹˜"],
            "ì‘ê¸‰": ["ì‘ê¸‰ì²˜ì¹˜", "ì‘ê¸‰ìƒí™©", "ì‘ê¸‰ì‹¤"],
            "ê³¨ì ˆ": ["ê³¨ì ˆ", "ì™¸ìƒ", "ì •í˜•ì™¸ê³¼"],
            "ì•½ë¬¼": ["ì•½ë¬¼", "ì²˜ë°©", "ë¶€ì‘ìš©"],
            "ìˆ˜ìˆ ": ["ìˆ˜ìˆ ", "ì‹œìˆ ", "ë§ˆì·¨"]
        }
        
        question_lower = question.lower()
        candidate_indices = set()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ í›„ë³´ ì„ ì •
        for keyword, related_terms in medical_keyword_map.items():
            if any(term in question_lower for term in related_terms):
                if keyword in self.embedding_index.get("keyword", {}):
                    candidate_indices.update(self.embedding_index["keyword"][keyword])
        
        # í›„ë³´ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì „ì²´ ê²€ìƒ‰
        if len(candidate_indices) < 10:
            return None
        
        return list(candidate_indices)
    
    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        a_np = np.array(a, dtype=np.float32)
        b_np = np.array(b, dtype=np.float32)
        
        norm_a = np.linalg.norm(a_np)
        norm_b = np.linalg.norm(b_np)
        
        if norm_a == 0 or norm_b == 0:
            return 0.0
        
        return float(np.dot(a_np, b_np) / (norm_a * norm_b))
    
    def _medical_relevance_filter(self, documents: List[Document], question: str) -> List[Document]:
        """ì˜ë£Œ ê´€ë ¨ì„± ê¸°ë°˜ ìµœì¢… í•„í„°ë§"""
        
        scored_docs = []
        question_words = set(question.lower().split())
        
        for doc in documents:
            score = doc.metadata.get("similarity_score", 0)
            
            # ì˜ë£Œ ê´€ë ¨ì„± ë³´ë„ˆìŠ¤
            medical_bonus = 0
            
            # 1. ì¹´í…Œê³ ë¦¬ ë³´ë„ˆìŠ¤
            category = doc.metadata.get("category", "")
            if any(cat in category for cat in ["ì‘ê¸‰", "ì¹˜ë£Œ", "ì•½ë¬¼", "ì§„ë‹¨"]):
                medical_bonus += 0.1
            
            # 2. í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
            keywords = doc.metadata.get("keywords", [])
            keyword_matches = len([kw for kw in keywords if kw.lower() in question_words])
            medical_bonus += keyword_matches * 0.05
            
            # 3. ì‹¬ê°ë„ ë³´ë„ˆìŠ¤
            severity = doc.metadata.get("severity", "medium")
            if severity == "critical" and any(word in question.lower() for word in ["ì‘ê¸‰", "ê¸‰ì„±", "ì¦‰ì‹œ"]):
                medical_bonus += 0.15
            
            # 4. ì‹ ë¢°ë„ ë³´ë„ˆìŠ¤
            confidence = doc.metadata.get("confidence", "medium")
            if confidence == "high":
                medical_bonus += 0.05
            
            final_score = score + medical_bonus
            scored_docs.append((final_score, doc))
        
        scored_docs.sort(reverse=True)
        return [doc for score, doc in scored_docs]
    
    def _update_document_index(self):
        """ê²€ìƒ‰ ì„±ëŠ¥ì„ ìœ„í•œ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("  ğŸ” ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        category_index = {}
        keyword_index = {}
        
        for i, doc in enumerate(self.medical_documents):
            # ì¹´í…Œê³ ë¦¬ ì¸ë±ìŠ¤
            category = doc.metadata.get("category", "ê¸°íƒ€")
            if category not in category_index:
                category_index[category] = []
            category_index[category].append(i)
            
            # í‚¤ì›Œë“œ ì¸ë±ìŠ¤ (ë‚´ìš©ì—ì„œ ì¶”ì¶œ)
            keywords = self._extract_keywords_from_content(doc.page_content)
            for keyword in keywords:
                if keyword not in keyword_index:
                    keyword_index[keyword] = []
                keyword_index[keyword].append(i)
        
        self.embedding_index = {
            "category": category_index,
            "keyword": keyword_index
        }
        
        print(f"    ğŸ“Š ì¹´í…Œê³ ë¦¬: {len(category_index)}ê°œ")
        print(f"    ğŸ·ï¸ í‚¤ì›Œë“œ: {len(keyword_index)}ê°œ")
    
    def _extract_keywords_from_content(self, content: str) -> List[str]:
        """ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì˜ë£Œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        medical_keywords = [
            "ë‹¹ë‡¨ë³‘", "ê³ í˜ˆì••", "ì‹¬ì •ì§€", "ì‘ê¸‰ì²˜ì¹˜", "ê³¨ì ˆ", "ë‚™ìƒ",
            "ì•½ë¬¼", "ì²˜ë°©", "ë¶€ì‘ìš©", "ìˆ˜ìˆ ", "ë§ˆì·¨", "ì§„ë‹¨", "ì¹˜ë£Œ",
            "ì¦ìƒ", "ì§ˆí™˜", "ë³‘ì›", "ì˜ì‚¬", "ê°„í˜¸ì‚¬", "í™˜ì"
        ]
        
        content_lower = content.lower()
        found_keywords = []
        
        for keyword in medical_keywords:
            if keyword in content_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def _update_search_stats(self, response_time: float):
        """ê²€ìƒ‰ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.search_stats["searches_performed"] += 1
        
        # í‰ê·  ì‘ë‹µì‹œê°„ ê³„ì‚°
        current_avg = self.search_stats["average_response_time"]
        search_count = self.search_stats["searches_performed"]
        
        new_avg = ((current_avg * (search_count - 1)) + response_time) / search_count
        self.search_stats["average_response_time"] = round(new_avg, 3)
    
    def _get_emergency_fallback_docs(self, question: str) -> List[Document]:
        """ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì‘ê¸‰ í´ë°± ë¬¸ì„œ"""
        return [
            Document(
                page_content=f"""
ì§ˆë¬¸: {question}

í˜„ì¬ ì˜ë£Œ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ì¼ë°˜ì ì¸ ì˜ë£Œ ì‘ê¸‰ìƒí™© ëŒ€ì‘ ì›ì¹™:
1. í™˜ì ì•ˆì „ ìµœìš°ì„  í™•ë³´
2. ìƒì²´ì§•í›„ í™•ì¸ (ì˜ì‹, í˜¸í¡, ë§¥ë°•, í˜ˆì••)
3. ì¦‰ì‹œ ì „ë¬¸ ì˜ë£Œì§„ê³¼ ìƒë‹´
4. ì‘ê¸‰ìƒí™© ì‹œ 119 ì‹ ê³ 

ì •í™•í•œ ì˜ë£Œ ì •ë³´ë¥¼ ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ ì˜ë£Œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
""",
                metadata={
                    "source": "emergency_fallback",
                    "category": "ì‘ê¸‰ì•ˆë‚´",
                    "confidence": "low",
                    "search_question": question,
                    "fallback_type": "emergency"
                }
            )
        ]
    
    def _create_fallback_web_doc(self, question: str) -> List[Document]:
        """ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í´ë°± ë¬¸ì„œ"""
        return [
            Document(
                page_content=f"""
ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {question}

ì˜ë£Œ ì •ë³´ëŠ” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ì—ì„œ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤:
- ì˜ë£Œì§„ê³¼ ì§ì ‘ ìƒë‹´
- ë³‘ì› ê³µì‹ ì›¹ì‚¬ì´íŠ¸
- ì •ë¶€ ë³´ê±´ ê¸°ê´€ ìë£Œ

í˜„ì¬ ë¡œì»¬ ì˜ë£Œ ë°ì´í„°ë² ì´ìŠ¤ë§Œ í™œìš©ë©ë‹ˆë‹¤.
""",
                metadata={
                    "source": "web_fallback", 
                    "category": "ê²€ìƒ‰ì‹¤íŒ¨",
                    "search_question": question,
                    "fallback_type": "web"
                }
            )
        ]
    
    def get_stats(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ê¸° ì „ì²´ í†µê³„"""
        cache_files = len(list(self.cache_dir.glob("*.pkl"))) if self.cache_enabled else 0
        
        loader_stats = self.document_loader.get_stats()
        
        return {
            "retriever_type": "RefactoredRetriever",
            "model_info": {
                "embedding_model": self.model_name,
                "dimensions": 3072
            },
            "document_stats": {
                "total_documents": len(self.medical_documents),
                "total_embeddings": len(self.document_embeddings),
                "index_categories": len(self.embedding_index.get("category", {})),
                "index_keywords": len(self.embedding_index.get("keyword", {}))
            },
            "search_performance": self.search_stats.copy(),
            "cache_info": {
                "cache_enabled": self.cache_enabled,
                "cache_files": cache_files,
                "cache_hit_rate": self.search_stats["cache_hits"] / max(1, self.search_stats["api_calls"] + self.search_stats["cache_hits"])
            },
            "cost_estimate": {
                "total_tokens": self.search_stats["total_tokens"],
                "estimated_cost_usd": self.search_stats["total_tokens"] * 0.13 / 1_000_000
            },
            "document_loader": loader_stats
        }
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        try:
            # íŒŒì¼ ìºì‹œ ì‚­ì œ
            if self.embeddings_file.exists():
                self.embeddings_file.unlink()
            if self.documents_file.exists():
                self.documents_file.unlink()
            
            # ë””ë ‰í† ë¦¬ ìºì‹œ ì‚­ì œ
            for cache_file in self.cache_dir.glob("*.pkl"):
                cache_file.unlink()
            
            # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            self.medical_documents = []
            self.document_embeddings = []
            self.embedding_index = {}
            
            print("ğŸ—‘ï¸ ëª¨ë“  ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")
            
        except Exception as e:
            print(f"âŒ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
    
    def reset_stats(self):
        """ê²€ìƒ‰ í†µê³„ ì´ˆê¸°í™”"""
        self.search_stats = {
            "api_calls": 0,
            "cache_hits": 0,
            "total_tokens": 0,
            "searches_performed": 0,
            "average_response_time": 0.0
        }
        
        # DocumentLoader í†µê³„ë„ ì´ˆê¸°í™”
        self.document_loader.reset_stats()
        
        print("ğŸ“Š ëª¨ë“  í†µê³„ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")

# í…ŒìŠ¤íŠ¸ ë° ì‚¬ìš© ì˜ˆì‹œ
def test_refactored_retriever():
    """ë¦¬íŒ©í† ë§ëœ ê²€ìƒ‰ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ë¦¬íŒ©í† ë§ëœ Retriever í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    try:
        # 1. ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        retriever = Retriever()
        
        # 2. ë¬¸ì„œ ë¡œë”© í…ŒìŠ¤íŠ¸
        if Path("./medical_docs").exists():
            print("ğŸ“š ë¬¸ì„œ ë¡œë”© í…ŒìŠ¤íŠ¸...")
            count = retriever.load_documents_from_directory("./medical_docs")
            print(f"âœ… {count}ê°œ ë¬¸ì„œ ë¡œë”© ì™„ë£Œ\n")
        
        # 3. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        test_queries = [
            "ë‹¹ë‡¨ë³‘ ì¹˜ë£Œ ë°©ë²•",
            "ì‘ê¸‰ì²˜ì¹˜ ì ˆì°¨",
            "ê³ í˜ˆì•• ì•½ë¬¼"
        ]
        
        for query in test_queries:
            print(f"ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: '{query}'")
            docs = retriever.retrieve_documents(query, k=3)
            print(f"   ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ")
            
            if docs:
                print(f"   ìµœê³  ìœ ì‚¬ë„: {docs[0].metadata.get('similarity_score', 0):.3f}")
            print()
        
        # 4. í†µê³„ ì¶œë ¥
        stats = retriever.get_stats()
        print("ğŸ“Š ê²€ìƒ‰ê¸° í†µê³„:")
        print(f"   ì´ ë¬¸ì„œ: {stats['document_stats']['total_documents']}ê°œ")
        print(f"   ê²€ìƒ‰ íšŸìˆ˜: {stats['search_performance']['searches_performed']}íšŒ")
        print(f"   í‰ê·  ì‘ë‹µì‹œê°„: {stats['search_performance']['average_response_time']}ì´ˆ")
        print(f"   ìºì‹œ ì ì¤‘ë¥ : {stats['cache_info']['cache_hit_rate']*100:.1f}%")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    test_refactored_retriever()