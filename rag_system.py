# rag_system.py
from typing import Literal, List, Dict, Any, Optional, Annotated
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver
import uuid
import os
from datetime import datetime
    
from config import Config
from components.local_retriever import LocalRetriever
from components.pubMed_searcher import PubMedSearcher
from components.evaluator import Evaluator
from components.generator import Generator
from components.integrator import Integrator
from components.output_formatter import OutputFormatter
from components.memory_manager import MemoryManager
from components.bedrock_retriever import BedrockRetriever

from components.parallel_searcher import ParallelSearcher


def use_last_value(current_val, new_val):
    """ë§ˆì§€ë§‰ ê°’ë§Œ ìœ ì§€í•˜ëŠ” ë¦¬ë“€ì„œ í•¨ìˆ˜"""
    return new_val

def append_messages(current_val: List[Dict[str, Any]], new_val: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ëŒ€í™” ë©”ì‹œì§€ë¥¼ ëˆ„ì í•˜ëŠ” ë¦¬ë“€ì„œ í•¨ìˆ˜"""
    if current_val is None:
        current_val = []
    if new_val is None:
        return current_val
    
    result = current_val.copy()
    if isinstance(new_val, list):
        result.extend(new_val)
    return result

class GraphState(BaseModel):
    """RAG ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” GraphState í´ë˜ìŠ¤"""
    question: Annotated[str, use_last_value]
    documents: List[Document] = []
    generation: Optional[str] = None
    rewrite_count: int = 0
    user_id: str = "default_user"

    conversation_history: Annotated[List[Dict[str, Any]], append_messages] = []
    generation_decision: Optional[str] = None
    hallucination_decision: Optional[str] = None
    
    source_categorized_docs: Dict[str, List[Document]] = {}
    integrated_answer: Optional[str] = None
    final_formatted_output: Dict[str, Any] = {}

    original_question: Optional[str] = None

class RAGSystem:
    """ë¦¬íŒ©í† ë§ëœ ì˜ë£Œ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.config = Config()
        self.llm = ChatOpenAI(model=self.config.MODEL_NAME, temperature=self.config.TEMPERATURE)

        # ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.evaluator = Evaluator(self.llm)
        self.generator = Generator(self.llm)
        self.integrator = Integrator(self.llm)
        self.output_formatter = OutputFormatter()
        self.memory_manager = MemoryManager(self.llm)

        # ë¡œì»¬ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.local_retriever = None
        if self.config.SEARCH_SOURCES_CONFIG.get("local", False):
            self.local_retriever = LocalRetriever(pubmed_searcher=self.pubmed_searcher)
            self.local_retriever.set_local_search_enabled(True)
            print("âœ… ë¡œì»¬ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

        # PubMed ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.pubmed_searcher = None
        if self.config.SEARCH_SOURCES_CONFIG.get("pubmed", False):
            self.pubmed_searcher = PubMedSearcher()
            print("âœ… PubMed ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
        # S3 ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.s3_retriever = None
        if self.config.SEARCH_SOURCES_CONFIG.get("s3", False):
            from components.s3_retriever import S3Retriever
            self.s3_retriever = S3Retriever(
            bucket_name="aws-medical-chatbot",
            search_function="medical-embedding-search",
            region_name="us-east-2" 
            )
            print("âœ… S3 ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
        # MedGemma ì´ˆê¸°í™”
        self.medgemma_searcher = None
        if self.config.SEARCH_SOURCES_CONFIG.get("medgemma", False):
            from components.medgemma_searcher import MedGemmaSearcher
            self.medgemma_searcher = MedGemmaSearcher()
            print("âœ… MedGemma ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
        # Tavily ì´ˆê¸°í™”
        self.tavily_searcher = None
        if self.config.SEARCH_SOURCES_CONFIG.get("tavily", False):
            from components.tavily_searcher import TavilySearcher
            self.tavily_searcher = TavilySearcher()
            print("âœ… Tavily ì›¹ ê²€ìƒ‰ ì´ˆê¸°í™” ì™„ë£Œ")

        # Bedrock Retriever ì¶”ê°€
        bedrock_kb_id = None
        bedrock_retriever = None
        try:
            if hasattr(self.config, 'BEDROCK_CONFIG'):
                bedrock_kb_id = self.config.BEDROCK_CONFIG.get("kb_id", "")
                if bedrock_kb_id:
                    print(f"ğŸ“ Bedrock KB ID í™•ì¸ë¨: {bedrock_kb_id}")
                    try:
                        from components.bedrock_retriever import BedrockRetriever
                        bedrock_retriever = BedrockRetriever(
                            kb_id=bedrock_kb_id,
                            region=self.config.BEDROCK_CONFIG.get("region", "us-east-1")
                        )
                        print("âœ… Bedrock Retriever ì´ˆê¸°í™” ì„±ê³µ")
                    except Exception as e:
                        import traceback
                        print(f"âš ï¸ Bedrock Retriever ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                        print(traceback.format_exc())
                        bedrock_retriever = None
                else:
                    print("â„¹ï¸ Bedrock KB IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        except Exception as e:
            print(f"âš ï¸ Bedrock ì„¤ì • í™•ì¸ ì‹¤íŒ¨: {str(e)}")

        self.bedrock_retriever = bedrock_retriever

        # ë³‘ë ¬ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.parallel_searcher = ParallelSearcher(
        local_retriever=self.local_retriever,
        s3_retriever=self.s3_retriever,
        medgemma_searcher=self.medgemma_searcher,
        pubmed_searcher=self.pubmed_searcher,
        tavily_searcher=self.tavily_searcher,
        bedrock_retriever=self.bedrock_retriever
    )
        
        # ì›Œí¬í”Œë¡œìš° ì„¤ì •
        self.workflow = None
        self.app = None
        self.checkpointer = MemorySaver()
        self._build_workflow()
    
    def _build_workflow(self):
        """ê°„ì†Œí™”ëœ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì„±"""
        self.workflow = StateGraph(GraphState)
        
        # í•µì‹¬ ë…¸ë“œë§Œ ì„¤ì • 
        self.workflow.add_node("process_question", self._process_question)
        self.workflow.add_node("parallel_search", self._parallel_search)
        self.workflow.add_node("integrate_answers", self._integrate_answers)
        self.workflow.add_node("hallucination_check", self._hallucination_check)
        self.workflow.add_node("format_output", self._format_output)

        # ì—£ì§€ ì„¤ì •
        self.workflow.set_entry_point("process_question")
        self.workflow.add_edge("process_question", "parallel_search")
        self.workflow.add_edge("parallel_search", "integrate_answers")
        self.workflow.add_edge("integrate_answers", "hallucination_check")

        self.workflow.add_conditional_edges(
            "hallucination_check",
            self._get_hallucination_decision,
            {"hallucination": "integrate_answers", "relevant": "format_output"} 
        )
        
        self.workflow.add_edge("format_output", END)
        
        # ê·¸ë˜í”„ ì»´íŒŒì¼
        self.app = self.workflow.compile(checkpointer=self.checkpointer)
    
    # ë…¸ë“œ í•¨ìˆ˜ë“¤
    def _process_question(self, state: GraphState) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬ ë° ë§¥ë½ ê¸°ë°˜ ì§ˆë¬¸ ì¬ìƒì„±"""
        print("==== [PROCESS QUESTION] ====")
        
        original_question = state.question
        current_history = state.conversation_history or []

        # 1ë‹¨ê³„: ë©”ëª¨ë¦¬ ê´€ë¦¬
        managed_history = self.memory_manager.manage_conversation_memory(current_history)
        
        # 2ë‹¨ê³„: ë§¥ë½ ê¸°ë°˜ ì§ˆë¬¸ ì¬ìƒì„± (ì´ì „ ëŒ€í™”ê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ì‹¤í–‰)
        enhanced_question = self.memory_manager.enhance_question_with_context(
            managed_history, original_question
        )
        
        # 3ë‹¨ê³„: ëŒ€í™” ì´ë ¥ì— ì›ë˜ ì§ˆë¬¸ ì¶”ê°€
        managed_history.append({
            "role": "user",
            "content": original_question,
            "timestamp": datetime.now().isoformat(),
            "enhanced_question": enhanced_question if enhanced_question != original_question else None
        })
        
        return {
            "conversation_history": managed_history,
            "question": enhanced_question,  # ì¬ìƒì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰/ë‹µë³€
            "original_question": original_question
        }
    
    def _retrieve(self, state: GraphState) -> Dict[str, Any]:
        """ë²¡í„° ê²€ìƒ‰ (ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš©)"""
        print("==== [VECTOR RETRIEVE] ====")
        documents = self.local_retriever.retrieve_documents(state.question)
        
        # ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš©
        threshold = getattr(self.config, 'SIMILARITY_THRESHOLD', 0.7)
        filtered_docs = []
        
        for doc in documents:
            similarity = doc.metadata.get("similarity_score", 0.0)
            if similarity >= threshold:
                filtered_docs.append(doc)
        
        print(f"ì„ê³„ê°’({threshold}) ì´ìƒ ë¬¸ì„œ: {len(filtered_docs)}ê°œ")
        return {"documents": filtered_docs}
        
    def _parallel_search(self, state: GraphState) -> Dict[str, Any]:
        
        if self.parallel_searcher:
            categorized_docs = self.parallel_searcher.search_all_parallel(state.question)
        else:
            # í´ë°±: ê¸°ë³¸ RAG ê²€ìƒ‰ë§Œ
            print("  âš ï¸ ë³‘ë ¬ ê²€ìƒ‰ê¸° ì—†ìŒ - RAGë§Œ ì‚¬ìš©")
            categorized_docs = {"rag": state.documents}
        
        print(f"ì†ŒìŠ¤ë³„ ë¬¸ì„œ ìˆ˜: {[(k, len(v)) for k, v in categorized_docs.items()]}")
        return {"source_categorized_docs": categorized_docs}
    
    def _integrate_answers(self, state: GraphState) -> Dict[str, Any]:
        """ê°€ì¤‘ì¹˜ ì ìš© ë‹µë³€ í†µí•©"""
        print("==== [INTEGRATE WITH WEIGHTS] ====")
        
        integrated_answer = self.integrator.integrate_answers(
            state.question, state.source_categorized_docs
        )
        
        # ëŒ€í™” ì´ë ¥ ì—…ë°ì´íŠ¸
        history = state.conversation_history.copy() if state.conversation_history else []
        history.append({
            "role": "assistant",
            "content": integrated_answer,
            "timestamp": datetime.now().isoformat()
        })
        
        return {
            "integrated_answer": integrated_answer,
            "generation": integrated_answer,
            "conversation_history": history
        }
    
    def _hallucination_check(self, state: GraphState) -> Dict[str, Any]:
        """í™˜ê° ê²€ì¶œ (ìµœëŒ€ 2íšŒ ì¬ì‹œë„)"""
        print("==== [HALLUCINATION CHECK] ====")
        
        # ì¬ì‹œë„ íšŸìˆ˜ í™•ì¸
        if state.rewrite_count >= 2:
            print("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬ - í˜„ì¬ ë‹µë³€ ì‚¬ìš©")
            return {"hallucination_decision": "relevant"}
        
        # ëª¨ë“  ë¬¸ì„œ ìˆ˜ì§‘
        all_docs = []
        for docs in state.source_categorized_docs.values():
            all_docs.extend(docs)
        
        decision = self.evaluator.check_hallucination(
            all_docs, state.generation, state.question
        )
        
        # í™˜ê° ê°ì§€ì‹œ ì¬ì‹œë„ ì¹´ìš´íŠ¸ ì¦ê°€
        if decision == "hallucination":
            print(f"í™˜ê° ê°ì§€ - ì¬ì‹œë„ {state.rewrite_count + 1}/2")
            return {
                "hallucination_decision": decision,
                "rewrite_count": state.rewrite_count + 1
            }
        
        return {"hallucination_decision": decision}
    
    def _get_hallucination_decision(self, state: GraphState) -> str:
        """í™˜ê° ê²°ì • ë°˜í™˜"""
        return state.hallucination_decision
    
    def _format_output(self, state: GraphState) -> Dict[str, Any]:
        """ìµœì¢… ì¶œë ¥ í¬ë§·íŒ…"""
        print("==== [FORMAT OUTPUT] ====")
        
        formatted_output = self.output_formatter.format_medical_answer(
            question=state.question,
            answer=state.integrated_answer,
            source_categorized_docs=state.source_categorized_docs,
            conversation_history=state.conversation_history,
            hallucination_attempts=state.rewrite_count + 1,
            original_question=state.original_question 
        )
        
        return {"final_formatted_output": formatted_output}
    
    def run_graph(self, question: str, user_id: str = None) -> Dict[str, Any]:
        """ê·¸ë˜í”„ ì‹¤í–‰ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)"""
        if not user_id:
            user_id = str(uuid.uuid4())
        
        initial_state = GraphState(question=question, user_id=user_id)
        
        config = {
            "configurable": {"thread_id": user_id},
            "recursion_limit": self.config.RECURSION_LIMIT
        }
        
        # ê¸°ì¡´ ëŒ€í™” ì´ë ¥ ë¡œë“œ
        existing_history = []
        try:
            checkpoint_tuple = self.checkpointer.get_tuple(config)
            if checkpoint_tuple:
                checkpoint = checkpoint_tuple.checkpoint
                if checkpoint and "channel_values" in checkpoint:
                    channel_values = checkpoint["channel_values"]
                    if "conversation_history" in channel_values:
                        existing_history = channel_values["conversation_history"] or []
        except Exception as e:
            print(f"ê¸°ì¡´ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        
        # ì´ˆê¸° ìƒíƒœì— ê¸°ì¡´ ëŒ€í™” í¬í•¨
        initial_state = GraphState(
            question=question,
            user_id=user_id,
            conversation_history=existing_history
        )
        
        # ê·¸ë˜í”„ ì‹¤í–‰
        result = self.app.invoke(initial_state, config=config)
        
        # ê²°ê³¼ ë°˜í™˜
        if "final_formatted_output" in result and result["final_formatted_output"]:
            formatted_output = result["final_formatted_output"]
            display_answer = self.output_formatter.format_for_display(formatted_output)
            
            return {
                "answer": display_answer,
                "raw_answer": result.get("generation", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."),
                "formatted_output": formatted_output,
                "user_id": user_id,
                "conversation_history": result.get("conversation_history", []),
                "source_breakdown": result.get("source_categorized_docs", {})
            }
        else:
            return {
                "answer": result["generation"] if "generation" in result else "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "user_id": user_id,
                "conversation_history": result.get("conversation_history", [])
            }
    
    def load_medical_documents(self, directory_path: str) -> int:
        """ì˜ë£Œ ë¬¸ì„œ ë¡œë“œ (í¸ì˜ ë©”ì„œë“œ)"""
        return self.retriever.load_documents_from_directory(directory_path)
    
    def refresh_components(self):
        """
        ì»´í¬ë„ŒíŠ¸ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ì—…ë°ì´íŠ¸ëœ í”„ë¡¬í”„íŠ¸ ì ìš©
        """
        print("==== [REFRESHING COMPONENTS] ====")
        
        try:
            # ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì¬ì´ˆê¸°í™”
            self.evaluator = Evaluator(self.llm)
            self.generator = Generator(self.llm)
            self.integrator = Integrator(self.llm)
            self.memory_manager = MemoryManager(self.llm)
            
            print("  âœ… ì»´í¬ë„ŒíŠ¸ ìƒˆë¡œê³ ì¹¨ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"  âŒ ì»´í¬ë„ŒíŠ¸ ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨: {str(e)}")
            return False

    def get_stats(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í†µê³„"""
        retriever_stats = self.local_retriever.get_stats()
        
        return {
            "workflow_nodes": 6,  # ê°„ì†Œí™”ëœ ë…¸ë“œ ìˆ˜
            **retriever_stats
        }
    
    def configure_search_sources(self, sources_config: Dict[str, bool]) -> Dict[str, bool]:
        """ê²€ìƒ‰ ì†ŒìŠ¤ ì„¤ì • ì—…ë°ì´íŠ¸"""
        print("==== [CONFIGURE SEARCH SOURCES] ====")
        
        # ë³‘ë ¬ ê²€ìƒ‰ê¸° ì†ŒìŠ¤ ì„¤ì •
        for source, enabled in sources_config.items():
            self.parallel_searcher.set_source_enabled(source, enabled)
        
        # í˜„ì¬ ì„¤ì • ë°˜í™˜
        return self.parallel_searcher.sources_enabled

    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ë° í†µê³„ ì¡°íšŒ"""
        status = {
            "search_sources": self.parallel_searcher.sources_enabled,
            "s3_stats": self.s3_retriever.get_stats() if self.s3_retriever else None,
            "medgemma_stats": self.medgemma_searcher.get_stats() if self.medgemma_searcher else None,
        }
        
        return status