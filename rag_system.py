# rag_system.py (ë¦¬íŒ©í† ë§ëœ ë²„ì „)
from typing import Literal, List, Dict, Any, Optional, Annotated
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver
import uuid
import os
from datetime import datetime
    
from config import Config
from components.router import Router
from components.retriever import Retriever
from components.evaluator import Evaluator
from components.generator import Generator
from components.integrator import Integrator
from components.output_formatter import OutputFormatter
from components.memory_manager import MemoryManager
from components.parallel_searcher import ParallelSearcher
from components.bedrock_retriever import BedrockRetriever


def use_last_value(current_val, new_val):
    """ë§ˆì§€ë§‰ ê°’ë§Œ ìœ ì§€í•˜ëŠ” ë¦¬ë“€ì„œ í•¨ìˆ˜"""
    return new_val

def append_messages(current_val: List[Dict[str, Any]], new_val: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ëŒ€í™” ë©”ì‹œì§€ë¥¼ ëˆ„ì í•˜ëŠ” ë¦¬ë“€ì„œ í•¨ìˆ˜"""
    if current_val is None:
        current_val = []
    if new_val is None:
        return current_val
    
    result = current_val.copy()
    if isinstance(new_val, list):
        result.extend(new_val)
    return result

class GraphState(BaseModel):
    """RAG ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” GraphState í´ë˜ìŠ¤"""
    question: Annotated[str, use_last_value]
    documents: List[Document] = []
    generation: Optional[str] = None
    rewrite_count: int = 0
    user_id: str = "default_user"

    conversation_history: Annotated[List[Dict[str, Any]], append_messages] = []
    routing_decision: Optional[str] = None
    generation_decision: Optional[str] = None
    hallucination_decision: Optional[str] = None
    
    source_categorized_docs: Dict[str, List[Document]] = {}
    integrated_answer: Optional[str] = None
    final_formatted_output: Dict[str, Any] = {}

    original_question: Optional[str] = None

class RAGSystem:
    """ë¦¬íŒ©í† ë§ëœ ì˜ë£Œ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.config = Config()
        self.llm = ChatOpenAI(model=self.config.MODEL_NAME, temperature=self.config.TEMPERATURE)

        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.router = Router(self.llm)
        self.retriever = Retriever()
        self.evaluator = Evaluator(self.llm)
        self.generator = Generator(self.llm)
        self.integrator = Integrator(self.llm)
        self.output_formatter = OutputFormatter()
        self.memory_manager = MemoryManager(self.llm)

        from components.medgemma_searcher import MedGemmaSearcher
        self.medgemma_searcher = None
        self.parallel_searcher = ParallelSearcher(self.retriever, self.medgemma_searcher)

        # Tavily ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        from components.tavily_searcher import TavilySearcher
        self.tavily_searcher = None
        try:
            self.tavily_searcher = TavilySearcher()
            print("âœ… Tavily ì›¹ ê²€ìƒ‰ ì¤€ë¹„ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ Tavily ì›¹ ê²€ìƒ‰ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        
        # S3 ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”
        from components.s3_retriever import S3Retriever
        self.s3_retriever = None
        """
        S3Retriever(
            bucket_name="aws-medical-chatbot",
            search_function="medical-embedding-search",
            region_name="us-east-2",  # ë¦¬ì „ íŒŒë¼ë¯¸í„° ì¶”ê°€
            enabled=True  # S3 ê²€ìƒ‰ ê¸°ë³¸ í™œì„±í™”
        )
        """

        # Bedrock Retriever ì¶”ê°€
        bedrock_kb_id = None
        bedrock_retriever = None
        try:
            if hasattr(self.config, 'BEDROCK_CONFIG'):
                bedrock_kb_id = self.config.BEDROCK_CONFIG.get("kb_id", "")
                if bedrock_kb_id:
                    print(f"ğŸ“ Bedrock KB ID í™•ì¸ë¨: {bedrock_kb_id}")
                    try:
                        from components.bedrock_retriever import BedrockRetriever
                        bedrock_retriever = BedrockRetriever(
                            kb_id=bedrock_kb_id,
                            region=self.config.BEDROCK_CONFIG.get("region", "us-east-1")
                        )
                        print("âœ… Bedrock Retriever ì´ˆê¸°í™” ì„±ê³µ")
                    except Exception as e:
                        import traceback
                        print(f"âš ï¸ Bedrock Retriever ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                        print(traceback.format_exc())
                        bedrock_retriever = None
                else:
                    print("â„¹ï¸ Bedrock KB IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        except Exception as e:
            print(f"âš ï¸ Bedrock ì„¤ì • í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        
        # í´ë˜ìŠ¤ì— í• ë‹¹
        self.bedrock_retriever = bedrock_retriever

        # ë³‘ë ¬ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.parallel_searcher = ParallelSearcher(
            self.retriever, 
            self.medgemma_searcher,
            self.tavily_searcher,
            self.s3_retriever,
            self.bedrock_retriever
        )
    
        # ì›Œí¬í”Œë¡œìš° ì„¤ì •
        self.workflow = None
        self.app = None
        self.checkpointer = MemorySaver()
        self._build_workflow()
    
    def _build_workflow(self):
        """ê°„ì†Œí™”ëœ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì„±"""
        self.workflow = StateGraph(GraphState)
        
        # í•µì‹¬ ë…¸ë“œë§Œ ì„¤ì • (7ê°œ)
        self.workflow.add_node("process_question", self._process_question)
        self.workflow.add_node("route_question", self._route_question)
        self.workflow.add_node("retrieve", self._retrieve)
        self.workflow.add_node("parallel_search", self._parallel_search)
        self.workflow.add_node("integrate_answers", self._integrate_answers)
        self.workflow.add_node("hallucination_check", self._hallucination_check)
        self.workflow.add_node("format_output", self._format_output)

        # ê°„ì†Œí™”ëœ ì—£ì§€ ì„¤ì •
        self.workflow.set_entry_point("process_question")
        self.workflow.add_edge("process_question", "route_question")
        
        self.workflow.add_conditional_edges(
            "route_question",
            self._get_route,
            {"web_search": "parallel_search", "vectorstore": "retrieve"}
        )
        
        self.workflow.add_edge("retrieve", "parallel_search")
        self.workflow.add_edge("parallel_search", "integrate_answers")
        self.workflow.add_edge("integrate_answers", "hallucination_check")
        
        self.workflow.add_conditional_edges(
            "hallucination_check",
            self._get_hallucination_decision,
            {"hallucination": "integrate_answers", "relevant": "format_output"} 
        )
        
        self.workflow.add_edge("format_output", END)
        
        # ê·¸ë˜í”„ ì»´íŒŒì¼
        self.app = self.workflow.compile(checkpointer=self.checkpointer)
    
    # ë…¸ë“œ í•¨ìˆ˜ë“¤ (ê°„ì†Œí™”ëœ ë²„ì „)
    def _process_question(self, state: GraphState) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬ ë° ë§¥ë½ ê¸°ë°˜ ì§ˆë¬¸ ì¬ìƒì„±"""
        print("==== [PROCESS QUESTION] ====")
        
        original_question = state.question
        current_history = state.conversation_history or []

        # 1ë‹¨ê³„: ë©”ëª¨ë¦¬ ê´€ë¦¬ (ìš”ì•½ ë“±)
        managed_history = self.memory_manager.manage_conversation_memory(current_history)
        
        # 2ë‹¨ê³„: ë§¥ë½ ê¸°ë°˜ ì§ˆë¬¸ ì¬ìƒì„± (ì´ì „ ëŒ€í™”ê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ì‹¤í–‰)
        enhanced_question = self.memory_manager.enhance_question_with_context(
            managed_history, original_question
        )
        
        # 3ë‹¨ê³„: ëŒ€í™” ì´ë ¥ì— ì›ë˜ ì§ˆë¬¸ ì¶”ê°€ (ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ í•œ ì§ˆë¬¸)
        managed_history.append({
            "role": "user",
            "content": original_question,
            "timestamp": datetime.now().isoformat(),
            "enhanced_question": enhanced_question if enhanced_question != original_question else None
        })
        
        return {
            "conversation_history": managed_history,
            "question": enhanced_question,  # ì¬ìƒì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰/ë‹µë³€
            "original_question": original_question
        }

    def _route_question(self, state: GraphState) -> Dict[str, Any]:
        """ì§ˆë¬¸ ë¼ìš°íŒ…"""
        print("==== [ROUTE QUESTION] ====")
        routing_decision = self.router.route_question(state.question)
        print(f"ë¼ìš°íŒ… ê²°ì •: {routing_decision}")
        return {"routing_decision": routing_decision}
    
    def _get_route(self, state: GraphState) -> str:
        """ë¼ìš°íŒ… ê²°ì • ë°˜í™˜"""
        return state.routing_decision
    
    def _retrieve(self, state: GraphState) -> Dict[str, Any]:
        """ë²¡í„° ê²€ìƒ‰ (ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš©)"""
        print("==== [VECTOR RETRIEVE] ====")
        documents = self.retriever.retrieve_documents(state.question)
        
        # ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš©
        threshold = getattr(self.config, 'SIMILARITY_THRESHOLD', 0.7)
        filtered_docs = []
        
        for doc in documents:
            similarity = doc.metadata.get("similarity_score", 0.0)
            if similarity >= threshold:
                filtered_docs.append(doc)
        
        print(f"ì„ê³„ê°’({threshold}) ì´ìƒ ë¬¸ì„œ: {len(filtered_docs)}ê°œ")
        return {"documents": filtered_docs}
        
    def _parallel_search(self, state: GraphState) -> Dict[str, Any]:
        
        if self.parallel_searcher:
            categorized_docs = self.parallel_searcher.search_all_parallel(state.question)
        else:
            # í´ë°±: ê¸°ë³¸ RAG ê²€ìƒ‰ë§Œ
            print("  âš ï¸ ë³‘ë ¬ ê²€ìƒ‰ê¸° ì—†ìŒ - RAGë§Œ ì‚¬ìš©")
            categorized_docs = {"rag": state.documents}
        
        print(f"ì†ŒìŠ¤ë³„ ë¬¸ì„œ ìˆ˜: {[(k, len(v)) for k, v in categorized_docs.items()]}")
        return {"source_categorized_docs": categorized_docs}
    
    def _integrate_answers(self, state: GraphState) -> Dict[str, Any]:
        """ê°€ì¤‘ì¹˜ ì ìš© ë‹µë³€ í†µí•©"""
        print("==== [INTEGRATE WITH WEIGHTS] ====")
        
        integrated_answer = self.integrator.integrate_answers(
            state.question, state.source_categorized_docs
        )
        
        # ëŒ€í™” ì´ë ¥ ì—…ë°ì´íŠ¸
        history = state.conversation_history.copy() if state.conversation_history else []
        history.append({
            "role": "assistant",
            "content": integrated_answer,
            "timestamp": datetime.now().isoformat()
        })
        
        return {
            "integrated_answer": integrated_answer,
            "generation": integrated_answer,
            "conversation_history": history
        }
    
    def _hallucination_check(self, state: GraphState) -> Dict[str, Any]:
        """í™˜ê° ê²€ì¶œ (ìµœëŒ€ 2íšŒ ì¬ì‹œë„)"""
        print("==== [HALLUCINATION CHECK] ====")
        
        # ì¬ì‹œë„ íšŸìˆ˜ í™•ì¸
        if state.rewrite_count >= 2:
            print("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬ - í˜„ì¬ ë‹µë³€ ì‚¬ìš©")
            return {"hallucination_decision": "relevant"}
        
        # ëª¨ë“  ë¬¸ì„œ ìˆ˜ì§‘
        all_docs = []
        for docs in state.source_categorized_docs.values():
            all_docs.extend(docs)
        
        decision = self.evaluator.check_hallucination(
            all_docs, state.generation, state.question
        )
        
        # í™˜ê° ê°ì§€ì‹œ ì¬ì‹œë„ ì¹´ìš´íŠ¸ ì¦ê°€
        if decision == "hallucination":
            print(f"í™˜ê° ê°ì§€ - ì¬ì‹œë„ {state.rewrite_count + 1}/2")
            return {
                "hallucination_decision": decision,
                "rewrite_count": state.rewrite_count + 1
            }
        
        return {"hallucination_decision": decision}
    
    def _get_hallucination_decision(self, state: GraphState) -> str:
        """í™˜ê° ê²°ì • ë°˜í™˜"""
        return state.hallucination_decision
    
    def _format_output(self, state: GraphState) -> Dict[str, Any]:
        """ìµœì¢… ì¶œë ¥ í¬ë§·íŒ…"""
        print("==== [FORMAT OUTPUT] ====")
        
        formatted_output = self.output_formatter.format_medical_answer(
            question=state.question,
            answer=state.integrated_answer,
            source_categorized_docs=state.source_categorized_docs,
            conversation_history=state.conversation_history,
            hallucination_attempts=state.rewrite_count + 1,
            original_question=state.original_question 
        )
        
        return {"final_formatted_output": formatted_output}
    
    def run_graph(self, question: str, user_id: str = None) -> Dict[str, Any]:
        """ê·¸ë˜í”„ ì‹¤í–‰ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)"""
        if not user_id:
            user_id = str(uuid.uuid4())
        
        initial_state = GraphState(question=question, user_id=user_id)
        
        config = {
            "configurable": {"thread_id": user_id},
            "recursion_limit": self.config.RECURSION_LIMIT
        }
        
        # ê¸°ì¡´ ëŒ€í™” ì´ë ¥ ë¡œë“œ
        existing_history = []
        try:
            checkpoint_tuple = self.checkpointer.get_tuple(config)
            if checkpoint_tuple:
                checkpoint = checkpoint_tuple.checkpoint
                if checkpoint and "channel_values" in checkpoint:
                    channel_values = checkpoint["channel_values"]
                    if "conversation_history" in channel_values:
                        existing_history = channel_values["conversation_history"] or []
        except Exception as e:
            print(f"ê¸°ì¡´ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        
        # ì´ˆê¸° ìƒíƒœì— ê¸°ì¡´ ëŒ€í™” í¬í•¨
        initial_state = GraphState(
            question=question,
            user_id=user_id,
            conversation_history=existing_history
        )
        
        # ê·¸ë˜í”„ ì‹¤í–‰
        result = self.app.invoke(initial_state, config=config)
        
        # ê²°ê³¼ ë°˜í™˜
        if "final_formatted_output" in result and result["final_formatted_output"]:
            formatted_output = result["final_formatted_output"]
            display_answer = self.output_formatter.format_for_display(formatted_output)
            
            return {
                "answer": display_answer,
                "raw_answer": result.get("generation", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."),
                "formatted_output": formatted_output,
                "user_id": user_id,
                "conversation_history": result.get("conversation_history", []),
                "source_breakdown": result.get("source_categorized_docs", {})
            }
        else:
            return {
                "answer": result["generation"] if "generation" in result else "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "user_id": user_id,
                "conversation_history": result.get("conversation_history", [])
            }
    
    def load_medical_documents(self, directory_path: str) -> int:
        """ì˜ë£Œ ë¬¸ì„œ ë¡œë“œ (í¸ì˜ ë©”ì„œë“œ)"""
        return self.retriever.load_documents_from_directory(directory_path)
    
    def refresh_components(self):
        """
        ì»´í¬ë„ŒíŠ¸ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ì—…ë°ì´íŠ¸ëœ í”„ë¡¬í”„íŠ¸ ì ìš©
        """
        print("==== [REFRESHING COMPONENTS] ====")
        
        try:
            # ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì¬ì´ˆê¸°í™”
            self.router = Router(self.llm)
            self.evaluator = Evaluator(self.llm)
            self.generator = Generator(self.llm)
            self.integrator = Integrator(self.llm)
            self.memory_manager = MemoryManager(self.llm)
            
            print("  âœ… ì»´í¬ë„ŒíŠ¸ ìƒˆë¡œê³ ì¹¨ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"  âŒ ì»´í¬ë„ŒíŠ¸ ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨: {str(e)}")
            return False

    def get_stats(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í†µê³„"""
        retriever_stats = self.retriever.get_stats()
        
        return {
            "workflow_nodes": 7,  # ê°„ì†Œí™”ëœ ë…¸ë“œ ìˆ˜
            **retriever_stats
        }
    
    def configure_search_sources(self, sources_config: Dict[str, bool]) -> Dict[str, bool]:
        """ê²€ìƒ‰ ì†ŒìŠ¤ ì„¤ì • ì—…ë°ì´íŠ¸"""
        print("==== [CONFIGURE SEARCH SOURCES] ====")
        
        # ë³‘ë ¬ ê²€ìƒ‰ê¸° ì†ŒìŠ¤ ì„¤ì •
        for source, enabled in sources_config.items():
            self.parallel_searcher.set_source_enabled(source, enabled)
        
        # í˜„ì¬ ì„¤ì • ë°˜í™˜
        return self.parallel_searcher.sources_enabled

    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ë° í†µê³„ ì¡°íšŒ"""
        status = {
            "search_sources": self.parallel_searcher.sources_enabled,
            "retriever_stats": self.retriever.get_stats(),
            "s3_stats": self.s3_retriever.get_stats() if self.s3_retriever else None,
            "medgemma_stats": self.medgemma_searcher.get_stats() if self.medgemma_searcher else None,
        }
        
        return status